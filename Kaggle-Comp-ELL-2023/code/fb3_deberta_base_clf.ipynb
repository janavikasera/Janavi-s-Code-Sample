{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b57c0b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Directory settings\n",
    "# ====================================================\n",
    "import os\n",
    "\n",
    "OUTPUT_DIR = './'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8eda197b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Sep 29 09:59:04 2022       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.64.00    Driver Version: 440.64.00    CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla P100-PCIE...  On   | 00000000:00:08.0 Off |                    0 |\r\n",
      "| N/A   27C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3855ca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# CFG\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    wandb=False\n",
    "    competition='FB3'\n",
    "    _wandb_kernel='nakama'\n",
    "    debug=False\n",
    "    apex=True\n",
    "    print_freq=20\n",
    "    num_workers=4\n",
    "    model=\"microsoft/deberta-v3-large\"\n",
    "    gradient_checkpointing=True\n",
    "    scheduler='cosine' # ['linear', 'cosine']\n",
    "    batch_scheduler=True\n",
    "    num_cycles=0.5\n",
    "    num_warmup_steps=0\n",
    "    epochs=4\n",
    "    encoder_lr=2e-5\n",
    "    decoder_lr=5e-5\n",
    "    min_lr=1e-6\n",
    "    eps=1e-6\n",
    "    betas=(0.9, 0.999)\n",
    "    batch_size=4\n",
    "    max_len=512\n",
    "    weight_decay=0.01\n",
    "    gradient_accumulation_steps=1\n",
    "    max_grad_norm=1000\n",
    "    target_cols=['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
    "    seed=42\n",
    "    n_fold=5\n",
    "    trn_fold=[0, 1, 2, 3, 4]\n",
    "    train=True\n",
    "    \n",
    "if CFG.debug:\n",
    "    CFG.epochs = 2\n",
    "    CFG.trn_fold = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8dfde852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizers.__version__: 0.12.1\n",
      "transformers.__version__: 4.21.2\n",
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import ast\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import string\n",
    "import pickle\n",
    "import random\n",
    "import joblib\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "# os.system('pip install iterative-stratification==0.1.7')\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# os.system('pip uninstall -y transformers')\n",
    "# os.system('pip uninstall -y tokenizers')\n",
    "# os.system('python -m pip install --no-index --find-links=../input/fb3-pip-wheels transformers')\n",
    "# os.system('python -m pip install --no-index --find-links=../input/fb3-pip-wheels tokenizers')\n",
    "import tokenizers\n",
    "import transformers\n",
    "print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n",
    "print(f\"transformers.__version__: {transformers.__version__}\")\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fd42145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Utils\n",
    "# ====================================================\n",
    "def MCRMSE(y_trues, y_preds):\n",
    "    scores = []\n",
    "    idxes = y_trues.shape[1]\n",
    "    \n",
    "    y_preds=np.array(y_preds)\n",
    "    for i in range(idxes):\n",
    "        y_true = y_trues[:,i]\n",
    "        y_pred = y_preds[:,i]\n",
    "        score = mean_squared_error(y_true, y_pred, squared=False) # RMSE\n",
    "        scores.append(score)\n",
    "    mcrmse_score = np.mean(scores)\n",
    "    return mcrmse_score, scores\n",
    "\n",
    "\n",
    "def get_score(y_trues, y_preds):\n",
    "    #label2score={0:1,1:1.5,2:2,3:2.5,4:3,5:3.5,6:4,7:4.5,8:5}\n",
    "    #scores=[[label2score[y_trues[j][i].item()] for j in range(6)] for i in range(y_trues.shape[1])]\n",
    "    \n",
    "    #scores=torch.tensor(scores,dtype=float)\n",
    "    \n",
    "    mcrmse_score, scores = MCRMSE(y_trues, y_preds)\n",
    "    return mcrmse_score, scores\n",
    "\n",
    "\n",
    "def get_logger(filename=OUTPUT_DIR+'train'):\n",
    "    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = get_logger()\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2792a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.shape: (3911, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>syntax</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>phraseology</th>\n",
       "      <th>grammar</th>\n",
       "      <th>conventions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0016926B079C</td>\n",
       "      <td>I think that students would benefit from learn...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0022683E9EA5</td>\n",
       "      <td>When a problem is a change you have to let it ...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00299B378633</td>\n",
       "      <td>Dear, Principal\\n\\nIf u change the school poli...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003885A45F42</td>\n",
       "      <td>The best time in life is when you become yours...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0049B1DF5CCC</td>\n",
       "      <td>Small act of kindness can impact in other peop...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id                                          full_text  cohesion  syntax  vocabulary  phraseology  grammar  conventions\n",
       "0  0016926B079C  I think that students would benefit from learn...       3.5     3.5         3.0          3.0      4.0          3.0\n",
       "1  0022683E9EA5  When a problem is a change you have to let it ...       2.5     2.5         3.0          2.0      2.0          2.5\n",
       "2  00299B378633  Dear, Principal\\n\\nIf u change the school poli...       3.0     3.5         3.0          3.0      3.0          2.5\n",
       "3  003885A45F42  The best time in life is when you become yours...       4.5     4.5         4.5          4.5      4.0          5.0\n",
       "4  0049B1DF5CCC  Small act of kindness can impact in other peop...       2.5     3.0         3.0          3.0      2.5          2.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.shape: (3, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000C359D63E</td>\n",
       "      <td>when a person has no experience on a job their...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000BAD50D026</td>\n",
       "      <td>Do you think students would benefit from being...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00367BB2546B</td>\n",
       "      <td>Thomas Jefferson once states that \"it is wonde...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id                                          full_text\n",
       "0  0000C359D63E  when a person has no experience on a job their...\n",
       "1  000BAD50D026  Do you think students would benefit from being...\n",
       "2  00367BB2546B  Thomas Jefferson once states that \"it is wonde..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission.shape: (3, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>syntax</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>phraseology</th>\n",
       "      <th>grammar</th>\n",
       "      <th>conventions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000C359D63E</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000BAD50D026</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00367BB2546B</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id  cohesion  syntax  vocabulary  phraseology  grammar  conventions\n",
       "0  0000C359D63E       3.0     3.0         3.0          3.0      3.0          3.0\n",
       "1  000BAD50D026       3.0     3.0         3.0          3.0      3.0          3.0\n",
       "2  00367BB2546B       3.0     3.0         3.0          3.0      3.0          3.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Data Loading\n",
    "# ====================================================\n",
    "train = pd.read_csv('../train.csv')\n",
    "test = pd.read_csv('../test.csv')\n",
    "submission = pd.read_csv('../sample_submission.csv')\n",
    "\n",
    "print(f\"train.shape: {train.shape}\")\n",
    "display(train.head())\n",
    "print(f\"test.shape: {test.shape}\")\n",
    "display(test.head())\n",
    "print(f\"submission.shape: {submission.shape}\")\n",
    "display(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b394a03e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold\n",
       "0    391\n",
       "1    391\n",
       "2    391\n",
       "3    391\n",
       "4    391\n",
       "5    391\n",
       "6    391\n",
       "7    391\n",
       "8    391\n",
       "9    392\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ====================================================\n",
    "# CV split\n",
    "# ====================================================\n",
    "Fold = MultilabelStratifiedKFold(n_splits=CFG.n_fold, shuffle=True, random_state=CFG.seed)\n",
    "for n, (train_index, val_index) in enumerate(Fold.split(train, train[CFG.target_cols])):\n",
    "    train.loc[val_index, 'fold'] = int(n)\n",
    "train['fold'] = train['fold'].astype(int)\n",
    "display(train.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d61ccbe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# tokenizer\n",
    "# ====================================================\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR+'tokenizer/')\n",
    "CFG.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b386d856",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "864de1fe9ebc439a93e44e985f9534e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3911 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_len: 1429\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Define max_len\n",
    "# ====================================================\n",
    "lengths = []\n",
    "tk0 = tqdm(train['full_text'].fillna(\"\").values, total=len(train))\n",
    "for text in tk0:\n",
    "    length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
    "    lengths.append(length)\n",
    "CFG.max_len = max(lengths) + 3 # cls & sep & sep\n",
    "LOGGER.info(f\"max_len: {CFG.max_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdf47c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Dataset\n",
    "# ====================================================\n",
    "def prepare_input(cfg, text):\n",
    "    inputs = cfg.tokenizer.encode_plus(\n",
    "        text, \n",
    "        return_tensors=None, \n",
    "        add_special_tokens=True, \n",
    "        max_length=CFG.max_len,\n",
    "        pad_to_max_length=True,\n",
    "        truncation=True\n",
    "    )\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype=torch.long)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, cfg, df):\n",
    "        self.cfg = cfg\n",
    "        self.texts = df['full_text'].values\n",
    "        self.labels = df[cfg.target_cols].values\n",
    "        self.score2label={1:0,1.5:1,2:2,2.5:3,\n",
    "                          3:4,3.5:5,4:6,4.5:7,5:8}\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        inputs = prepare_input(self.cfg, self.texts[item])\n",
    "        label0 = torch.tensor(self.score2label[self.labels[item][0]])\n",
    "        label1 = torch.tensor(self.score2label[self.labels[item][1]])\n",
    "        label2 = torch.tensor(self.score2label[self.labels[item][2]])\n",
    "        label3 = torch.tensor(self.score2label[self.labels[item][3]])\n",
    "        label4 = torch.tensor(self.score2label[self.labels[item][4]])\n",
    "        label5 = torch.tensor(self.score2label[self.labels[item][5]])\n",
    "        return inputs,label0, label1,label2,label3,label4,label5\n",
    "    \n",
    "\n",
    "def collate(inputs):\n",
    "    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k][:,:mask_len]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af03e5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Model\n",
    "# ====================================================\n",
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "        \n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "    \n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg, config_path=None, pretrained=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        if config_path is None:\n",
    "            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)\n",
    "            self.config.hidden_dropout = 0.\n",
    "            self.config.hidden_dropout_prob = 0.\n",
    "            self.config.attention_dropout = 0.\n",
    "            self.config.attention_probs_dropout_prob = 0.\n",
    "            LOGGER.info(self.config)\n",
    "        else:\n",
    "            self.config = torch.load(config_path)\n",
    "        if pretrained:\n",
    "            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)\n",
    "        else:\n",
    "            self.model = AutoModel(self.config)\n",
    "        if self.cfg.gradient_checkpointing:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "        self.pool = MeanPooling()\n",
    "        \n",
    "        self.fc0 = nn.Linear(self.config.hidden_size, 9)\n",
    "        self.fc1 = nn.Linear(self.config.hidden_size, 9)\n",
    "        self.fc2 = nn.Linear(self.config.hidden_size, 9)\n",
    "        self.fc3 = nn.Linear(self.config.hidden_size, 9)\n",
    "        self.fc4 = nn.Linear(self.config.hidden_size, 9)\n",
    "        self.fc5 = nn.Linear(self.config.hidden_size, 9)\n",
    "        \n",
    "        self._init_weights(self.fc0)\n",
    "        self._init_weights(self.fc1)\n",
    "        self._init_weights(self.fc2)\n",
    "        self._init_weights(self.fc3)\n",
    "        self._init_weights(self.fc4)\n",
    "        self._init_weights(self.fc5)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        \n",
    "    def feature(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_states = outputs[0]\n",
    "        feature = self.pool(last_hidden_states, inputs['attention_mask'])\n",
    "        return feature\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        feature = self.feature(inputs)\n",
    "        \n",
    "        output0 = self.fc0(feature)\n",
    "        output1 = self.fc1(feature)\n",
    "        output2 = self.fc2(feature)\n",
    "        output3 = self.fc3(feature)\n",
    "        output4 = self.fc4(feature)\n",
    "        output5 = self.fc5(feature)\n",
    "        \n",
    "        \n",
    "        output=[output0,output1,output2,output3,output4,output5]\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b852e9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Loss\n",
    "# ====================================================\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self, reduction='mean', eps=1e-9):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss(reduction='none')\n",
    "        self.reduction = reduction\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        loss = torch.sqrt(self.mse(y_pred, y_true) + self.eps)\n",
    "        if self.reduction == 'none':\n",
    "            loss = loss\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc370e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Helper functions\n",
    "# ====================================================\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    for step, (inputs, labels0,labels1,labels2,labels3,labels4,labels5) in enumerate(train_loader):\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels0 = labels0.to(device)\n",
    "        labels1 = labels1.to(device)\n",
    "        labels2 = labels2.to(device)\n",
    "        labels3 = labels3.to(device)\n",
    "        labels4 = labels4.to(device)\n",
    "        labels5 = labels5.to(device)\n",
    "        batch_size = labels0.size(0)\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "            y_preds = model(inputs)\n",
    "            loss_0 = criterion(y_preds[0], labels0)\n",
    "            loss_1 = criterion(y_preds[1], labels1)\n",
    "            loss_2 = criterion(y_preds[2], labels2)\n",
    "            loss_3 = criterion(y_preds[3], labels3)\n",
    "            loss_4 = criterion(y_preds[4], labels4)\n",
    "            loss_5 = criterion(y_preds[5], labels5)\n",
    "            \n",
    "            loss=(loss_0+loss_1+loss_2+loss_3+loss_4+loss_5)/6\n",
    "            \n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "            if CFG.batch_scheduler:\n",
    "                scheduler.step()\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.8f}  '\n",
    "                  .format(epoch+1, step, len(train_loader), \n",
    "                          remain=timeSince(start, float(step+1)/len(train_loader)),\n",
    "                          loss=losses,\n",
    "                          grad_norm=grad_norm,\n",
    "                          lr=scheduler.get_lr()[0]))\n",
    "        if CFG.wandb:\n",
    "            wandb.log({f\"[fold{fold}] loss\": losses.val,\n",
    "                       f\"[fold{fold}] lr\": scheduler.get_lr()[0]})\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "    label2score = {v:k for k,v in {1:0,1.5:1,2:2,2.5:3,\n",
    "                          3:4,3.5:5,4:6,4.5:7,5:8}.items()}\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    start = end = time.time()\n",
    "    for step, (inputs,  labels0,labels1,labels2,labels3,labels4,labels5) in enumerate(valid_loader):\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels0 = labels0.to(device)\n",
    "        labels1 = labels1.to(device)\n",
    "        labels2 = labels2.to(device)\n",
    "        labels3 = labels3.to(device)\n",
    "        labels4 = labels4.to(device)\n",
    "        labels5 = labels5.to(device)\n",
    "        batch_size = labels0.size(0)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "            loss_0 = criterion(y_preds[0], labels0)\n",
    "            loss_1 = criterion(y_preds[1], labels1)\n",
    "            loss_2 = criterion(y_preds[2], labels2)\n",
    "            loss_3 = criterion(y_preds[3], labels3)\n",
    "            loss_4 = criterion(y_preds[4], labels4)\n",
    "            loss_5 = criterion(y_preds[5], labels5)\n",
    "            \n",
    "            loss=(loss_0+loss_1+loss_2+loss_3+loss_4+loss_5)/6\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        \n",
    "        y_preds=torch.cat(y_preds,dim=1).reshape([-1,6,9])\n",
    "        \n",
    "        y_preds=torch.argmax(y_preds,dim=2).cpu().numpy()\n",
    "    \n",
    "        y_preds=[[label2score[x] for x in y] for y in y_preds]\n",
    "        predictions+=y_preds\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(step, len(valid_loader),\n",
    "                          loss=losses,\n",
    "                          remain=timeSince(start, float(step+1)/len(valid_loader))))\n",
    "    #predictions = np.concatenate(preds)\n",
    "    \n",
    "    return losses.avg, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7d89b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Helper functions\n",
    "# ====================================================\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    for step, (inputs, labels0,labels1,labels2,labels3,labels4,labels5) in enumerate(train_loader):\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels0 = labels0.to(device)\n",
    "        labels1 = labels1.to(device)\n",
    "        labels2 = labels2.to(device)\n",
    "        labels3 = labels3.to(device)\n",
    "        labels4 = labels4.to(device)\n",
    "        labels5 = labels5.to(device)\n",
    "        batch_size = labels0.size(0)\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "            y_preds = model(inputs)\n",
    "            loss_0 = criterion(y_preds[0], labels0)\n",
    "            loss_1 = criterion(y_preds[1], labels1)\n",
    "            loss_2 = criterion(y_preds[2], labels2)\n",
    "            loss_3 = criterion(y_preds[3], labels3)\n",
    "            loss_4 = criterion(y_preds[4], labels4)\n",
    "            loss_5 = criterion(y_preds[5], labels5)\n",
    "            \n",
    "            loss=(loss_0+loss_1+loss_2+loss_3+loss_4+loss_5)/6\n",
    "            \n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            global_step += 1\n",
    "            if CFG.batch_scheduler:\n",
    "                scheduler.step()\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.8f}  '\n",
    "                  .format(epoch+1, step, len(train_loader), \n",
    "                          remain=timeSince(start, float(step+1)/len(train_loader)),\n",
    "                          loss=losses,\n",
    "                          grad_norm=grad_norm,\n",
    "                          lr=scheduler.get_lr()[0]))\n",
    "        if CFG.wandb:\n",
    "            wandb.log({f\"[fold{fold}] loss\": losses.val,\n",
    "                       f\"[fold{fold}] lr\": scheduler.get_lr()[0]})\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "    label2score = {v:k for k,v in {1:0,1.5:1,2:2,2.5:3,\n",
    "                          3:4,3.5:5,4:6,4.5:7,5:8}.items()}\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    start = end = time.time()\n",
    "    for step, (inputs,  labels0,labels1,labels2,labels3,labels4,labels5) in enumerate(valid_loader):\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        labels0 = labels0.to(device)\n",
    "        labels1 = labels1.to(device)\n",
    "        labels2 = labels2.to(device)\n",
    "        labels3 = labels3.to(device)\n",
    "        labels4 = labels4.to(device)\n",
    "        labels5 = labels5.to(device)\n",
    "        batch_size = labels0.size(0)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "            loss_0 = criterion(y_preds[0], labels0)\n",
    "            loss_1 = criterion(y_preds[1], labels1)\n",
    "            loss_2 = criterion(y_preds[2], labels2)\n",
    "            loss_3 = criterion(y_preds[3], labels3)\n",
    "            loss_4 = criterion(y_preds[4], labels4)\n",
    "            loss_5 = criterion(y_preds[5], labels5)\n",
    "            \n",
    "            loss=(loss_0+loss_1+loss_2+loss_3+loss_4+loss_5)/6\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        \n",
    "        y_preds=torch.cat(y_preds,dim=1).reshape([-1,6,9])\n",
    "        \n",
    "        y_preds=torch.argmax(y_preds,dim=2).cpu().numpy()\n",
    "    \n",
    "        y_preds=[[label2score[x] for x in y] for y in y_preds]\n",
    "        predictions+=y_preds\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(step, len(valid_loader),\n",
    "                          loss=losses,\n",
    "                          remain=timeSince(start, float(step+1)/len(valid_loader))))\n",
    "    #predictions = np.concatenate(preds)\n",
    "    \n",
    "    return losses.avg, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b732b567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# train loop\n",
    "# ====================================================\n",
    "def train_loop(folds, fold):\n",
    "    \n",
    "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "    train_folds = folds[folds['fold'] != fold].reset_index(drop=True)\n",
    "    valid_folds = folds[folds['fold'] == fold].reset_index(drop=True)\n",
    "    valid_labels = valid_folds[CFG.target_cols].values\n",
    "    \n",
    "    train_dataset = TrainDataset(CFG, train_folds)\n",
    "    valid_dataset = TrainDataset(CFG, valid_folds)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
    "    \n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=CFG.batch_size * 2,\n",
    "                              shuffle=False,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "    model = CustomModel(CFG, config_path=None, pretrained=True)\n",
    "    torch.save(model.config, OUTPUT_DIR+'config.pth')\n",
    "    model.to(device)\n",
    "    \n",
    "    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "             'lr': decoder_lr, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        return optimizer_parameters\n",
    "\n",
    "    optimizer_parameters = get_optimizer_params(model,\n",
    "                                                encoder_lr=CFG.encoder_lr, \n",
    "                                                decoder_lr=CFG.decoder_lr,\n",
    "                                                weight_decay=CFG.weight_decay)\n",
    "    optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)\n",
    "    \n",
    "    # ====================================================\n",
    "    # scheduler\n",
    "    # ====================================================\n",
    "    def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "        if cfg.scheduler == 'linear':\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n",
    "            )\n",
    "        elif cfg.scheduler == 'cosine':\n",
    "            scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n",
    "            )\n",
    "        return scheduler\n",
    "    \n",
    "    num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n",
    "    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n",
    "\n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    #criterion = nn.SmoothL1Loss(reduction='mean') # RMSELoss(reduction=\"mean\")\n",
    "    criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "    \n",
    "    best_score = np.inf\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # train\n",
    "        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n",
    "\n",
    "        # eval\n",
    "        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, device)\n",
    "        #print(valid_labels)\n",
    "        #print(predictions)\n",
    "        # scoring\n",
    "        score, scores = get_score(valid_labels, predictions)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}  Scores: {scores}')\n",
    "        if CFG.wandb:\n",
    "            wandb.log({f\"[fold{fold}] epoch\": epoch+1, \n",
    "                       f\"[fold{fold}] avg_train_loss\": avg_loss, \n",
    "                       f\"[fold{fold}] avg_val_loss\": avg_val_loss,\n",
    "                       f\"[fold{fold}] score\": score})\n",
    "        \n",
    "        if best_score > score:\n",
    "            best_score = score\n",
    "            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save({'model': model.state_dict(),\n",
    "                        'predictions': predictions},\n",
    "                        OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\")\n",
    "\n",
    "    predictions = torch.load(OUTPUT_DIR+f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\", \n",
    "                             map_location=torch.device('cpu'))['predictions']\n",
    "    valid_folds[[f\"pred_{c}\" for c in CFG.target_cols]] = predictions\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return valid_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f9175e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 6 training ==========\n",
      "DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-large\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 1024,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/880] Elapsed 0m 3s (remain 45m 11s) Loss: 2.2467(2.2467) Grad: inf  LR: 0.00002000  \n",
      "Epoch: [1][20/880] Elapsed 0m 45s (remain 30m 58s) Loss: 1.7687(1.9939) Grad: 102374.2031  LR: 0.00002000  \n",
      "Epoch: [1][40/880] Elapsed 1m 34s (remain 32m 7s) Loss: 1.5127(1.8869) Grad: 108722.8203  LR: 0.00001999  \n",
      "Epoch: [1][60/880] Elapsed 2m 22s (remain 31m 59s) Loss: 1.4727(1.7938) Grad: 117950.7266  LR: 0.00001999  \n",
      "Epoch: [1][80/880] Elapsed 3m 10s (remain 31m 21s) Loss: 1.7227(1.7477) Grad: 121807.5078  LR: 0.00001997  \n",
      "Epoch: [1][100/880] Elapsed 3m 55s (remain 30m 17s) Loss: 1.4935(1.6997) Grad: 207197.7031  LR: 0.00001996  \n",
      "Epoch: [1][120/880] Elapsed 4m 45s (remain 29m 49s) Loss: 1.5196(1.6703) Grad: 160495.7812  LR: 0.00001994  \n",
      "Epoch: [1][140/880] Elapsed 5m 32s (remain 29m 2s) Loss: 3.0056(1.6478) Grad: 204519.3281  LR: 0.00001992  \n",
      "Epoch: [1][160/880] Elapsed 6m 23s (remain 28m 33s) Loss: 1.5546(1.6365) Grad: 131829.5469  LR: 0.00001990  \n",
      "Epoch: [1][180/880] Elapsed 7m 12s (remain 27m 51s) Loss: 1.6766(1.6256) Grad: 155898.0312  LR: 0.00001987  \n",
      "Epoch: [1][200/880] Elapsed 7m 56s (remain 26m 49s) Loss: 1.8430(1.6086) Grad: 191054.5312  LR: 0.00001984  \n",
      "Epoch: [1][220/880] Elapsed 8m 47s (remain 26m 14s) Loss: 1.2773(1.5881) Grad: 115619.3203  LR: 0.00001981  \n",
      "Epoch: [1][240/880] Elapsed 9m 36s (remain 25m 28s) Loss: 1.1474(1.5747) Grad: 98119.7422  LR: 0.00001977  \n",
      "Epoch: [1][260/880] Elapsed 10m 24s (remain 24m 41s) Loss: 1.5284(1.5668) Grad: 131220.8125  LR: 0.00001973  \n",
      "Epoch: [1][280/880] Elapsed 11m 22s (remain 24m 15s) Loss: 1.5066(1.5594) Grad: 111671.9531  LR: 0.00001969  \n",
      "Epoch: [1][300/880] Elapsed 12m 1s (remain 23m 8s) Loss: 1.2815(1.5512) Grad: 107912.0078  LR: 0.00001964  \n",
      "Epoch: [1][320/880] Elapsed 12m 51s (remain 22m 24s) Loss: 1.1279(1.5424) Grad: 101033.3594  LR: 0.00001959  \n",
      "Epoch: [1][340/880] Elapsed 13m 44s (remain 21m 42s) Loss: 1.3709(1.5358) Grad: 106663.0078  LR: 0.00001954  \n",
      "Epoch: [1][360/880] Elapsed 14m 31s (remain 20m 53s) Loss: 1.4006(1.5257) Grad: 118529.8750  LR: 0.00001949  \n",
      "Epoch: [1][380/880] Elapsed 15m 18s (remain 20m 2s) Loss: 1.4905(1.5183) Grad: 173341.7812  LR: 0.00001943  \n",
      "Epoch: [1][400/880] Elapsed 16m 7s (remain 19m 16s) Loss: 1.3776(1.5184) Grad: 110429.4297  LR: 0.00001937  \n",
      "Epoch: [1][420/880] Elapsed 16m 55s (remain 18m 26s) Loss: 1.2675(1.5139) Grad: 120934.0469  LR: 0.00001930  \n",
      "Epoch: [1][440/880] Elapsed 17m 39s (remain 17m 34s) Loss: 1.3768(1.5100) Grad: 116476.9922  LR: 0.00001924  \n",
      "Epoch: [1][460/880] Elapsed 18m 26s (remain 16m 46s) Loss: 1.8276(1.5100) Grad: 183105.9375  LR: 0.00001917  \n",
      "Epoch: [1][480/880] Elapsed 19m 10s (remain 15m 54s) Loss: 1.4087(1.5094) Grad: 108485.7266  LR: 0.00001909  \n",
      "Epoch: [1][500/880] Elapsed 19m 55s (remain 15m 4s) Loss: 1.3384(1.5072) Grad: 109834.3281  LR: 0.00001902  \n",
      "Epoch: [1][520/880] Elapsed 20m 47s (remain 14m 19s) Loss: 1.2867(1.5026) Grad: 101271.5703  LR: 0.00001894  \n",
      "Epoch: [1][540/880] Elapsed 21m 30s (remain 13m 28s) Loss: 1.2448(1.5033) Grad: 122181.4297  LR: 0.00001886  \n",
      "Epoch: [1][560/880] Elapsed 22m 24s (remain 12m 44s) Loss: 1.3123(1.5004) Grad: 97715.2969  LR: 0.00001877  \n",
      "Epoch: [1][580/880] Elapsed 23m 8s (remain 11m 54s) Loss: 1.2631(1.4994) Grad: 81194.3438  LR: 0.00001869  \n",
      "Epoch: [1][600/880] Elapsed 23m 50s (remain 11m 4s) Loss: 1.6294(1.4974) Grad: 141698.4062  LR: 0.00001860  \n",
      "Epoch: [1][620/880] Elapsed 24m 38s (remain 10m 16s) Loss: 1.2058(1.4930) Grad: 106001.2266  LR: 0.00001850  \n",
      "Epoch: [1][640/880] Elapsed 25m 26s (remain 9m 29s) Loss: 1.2167(1.4921) Grad: 85971.6641  LR: 0.00001841  \n",
      "Epoch: [1][660/880] Elapsed 26m 15s (remain 8m 42s) Loss: 1.5444(1.4887) Grad: 152004.8438  LR: 0.00001831  \n",
      "Epoch: [1][680/880] Elapsed 26m 59s (remain 7m 53s) Loss: 1.4866(1.4852) Grad: 148626.3750  LR: 0.00001821  \n",
      "Epoch: [1][700/880] Elapsed 27m 45s (remain 7m 5s) Loss: 1.4269(1.4805) Grad: 144561.0000  LR: 0.00001811  \n",
      "Epoch: [1][720/880] Elapsed 28m 27s (remain 6m 16s) Loss: 1.3110(1.4763) Grad: 95956.0859  LR: 0.00001800  \n",
      "Epoch: [1][740/880] Elapsed 29m 11s (remain 5m 28s) Loss: 1.3267(1.4727) Grad: 103673.1016  LR: 0.00001789  \n",
      "Epoch: [1][760/880] Elapsed 29m 57s (remain 4m 41s) Loss: 1.2687(1.4698) Grad: 94331.2422  LR: 0.00001778  \n",
      "Epoch: [1][780/880] Elapsed 30m 40s (remain 3m 53s) Loss: 1.2768(1.4671) Grad: 88472.9844  LR: 0.00001767  \n",
      "Epoch: [1][800/880] Elapsed 31m 25s (remain 3m 5s) Loss: 1.6765(1.4672) Grad: 104516.3438  LR: 0.00001755  \n",
      "Epoch: [1][820/880] Elapsed 32m 16s (remain 2m 19s) Loss: 1.6540(1.4653) Grad: 121943.5859  LR: 0.00001743  \n",
      "Epoch: [1][840/880] Elapsed 33m 7s (remain 1m 32s) Loss: 1.5062(1.4637) Grad: 136443.7969  LR: 0.00001731  \n",
      "Epoch: [1][860/880] Elapsed 34m 9s (remain 0m 45s) Loss: 1.4215(1.4607) Grad: 102597.0938  LR: 0.00001719  \n",
      "Epoch: [1][879/880] Elapsed 34m 52s (remain 0m 0s) Loss: 1.7013(1.4602) Grad: 98800.9297  LR: 0.00001707  \n",
      "EVAL: [0/49] Elapsed 0m 3s (remain 2m 52s) Loss: 1.2081(1.2081) \n",
      "EVAL: [20/49] Elapsed 0m 36s (remain 0m 48s) Loss: 1.1678(1.3776) \n",
      "EVAL: [40/49] Elapsed 1m 1s (remain 0m 11s) Loss: 1.4973(1.3873) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 1.4602  avg_val_loss: 1.3837  time: 2167s\n",
      "Epoch 1 - Score: 0.5172  Scores: [0.5452656031743751, 0.5218989525032992, 0.507614651355829, 0.531609533071195, 0.506984465325952, 0.48966296671472614]\n",
      "Epoch 1 - Save Best Score: 0.5172 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [48/49] Elapsed 1m 14s (remain 0m 0s) Loss: 1.1736(1.3837) \n",
      "Epoch: [2][0/880] Elapsed 0m 2s (remain 31m 47s) Loss: 1.4607(1.4607) Grad: 468829.0000  LR: 0.00001706  \n",
      "Epoch: [2][20/880] Elapsed 0m 48s (remain 33m 17s) Loss: 1.3615(1.2917) Grad: 371829.4375  LR: 0.00001694  \n",
      "Epoch: [2][40/880] Elapsed 1m 33s (remain 31m 44s) Loss: 1.5761(1.2771) Grad: 536649.6250  LR: 0.00001681  \n",
      "Epoch: [2][60/880] Elapsed 2m 20s (remain 31m 20s) Loss: 1.3906(1.2588) Grad: 451200.0938  LR: 0.00001668  \n",
      "Epoch: [2][80/880] Elapsed 3m 12s (remain 31m 36s) Loss: 1.2259(1.2571) Grad: 333426.9375  LR: 0.00001654  \n",
      "Epoch: [2][100/880] Elapsed 3m 58s (remain 30m 40s) Loss: 1.4719(1.2528) Grad: 379628.4375  LR: 0.00001641  \n",
      "Epoch: [2][120/880] Elapsed 4m 48s (remain 30m 11s) Loss: 1.1913(1.2500) Grad: 211797.6719  LR: 0.00001627  \n",
      "Epoch: [2][140/880] Elapsed 5m 35s (remain 29m 20s) Loss: 1.2846(1.2498) Grad: 228954.0781  LR: 0.00001613  \n",
      "Epoch: [2][160/880] Elapsed 6m 26s (remain 28m 46s) Loss: 1.2131(1.2536) Grad: 204810.4062  LR: 0.00001599  \n",
      "Epoch: [2][180/880] Elapsed 7m 13s (remain 27m 55s) Loss: 1.1350(1.2528) Grad: 219579.1719  LR: 0.00001584  \n",
      "Epoch: [2][200/880] Elapsed 8m 0s (remain 27m 4s) Loss: 1.3071(1.2515) Grad: 232463.8125  LR: 0.00001570  \n",
      "Epoch: [2][220/880] Elapsed 8m 52s (remain 26m 29s) Loss: 1.2838(1.2566) Grad: 224249.0625  LR: 0.00001555  \n",
      "Epoch: [2][240/880] Elapsed 9m 36s (remain 25m 27s) Loss: 1.2363(1.2580) Grad: 214258.5469  LR: 0.00001540  \n",
      "Epoch: [2][260/880] Elapsed 10m 21s (remain 24m 34s) Loss: 1.2695(1.2572) Grad: 193282.6094  LR: 0.00001525  \n",
      "Epoch: [2][280/880] Elapsed 11m 9s (remain 23m 46s) Loss: 1.1284(1.2632) Grad: 183504.9062  LR: 0.00001510  \n",
      "Epoch: [2][300/880] Elapsed 12m 1s (remain 23m 8s) Loss: 1.7273(1.2685) Grad: 208432.3438  LR: 0.00001494  \n",
      "Epoch: [2][320/880] Elapsed 12m 43s (remain 22m 9s) Loss: 1.0437(1.2650) Grad: 217367.7500  LR: 0.00001478  \n",
      "Epoch: [2][340/880] Elapsed 13m 36s (remain 21m 30s) Loss: 1.3965(1.2664) Grad: 296679.3438  LR: 0.00001463  \n",
      "Epoch: [2][360/880] Elapsed 14m 25s (remain 20m 44s) Loss: 1.1313(1.2656) Grad: 186429.0625  LR: 0.00001447  \n",
      "Epoch: [2][380/880] Elapsed 15m 10s (remain 19m 52s) Loss: 1.1189(1.2636) Grad: 203830.1562  LR: 0.00001431  \n",
      "Epoch: [2][400/880] Elapsed 16m 4s (remain 19m 12s) Loss: 1.7043(1.2642) Grad: 296193.1250  LR: 0.00001415  \n",
      "Epoch: [2][420/880] Elapsed 16m 52s (remain 18m 24s) Loss: 1.2236(1.2612) Grad: 178360.3281  LR: 0.00001398  \n",
      "Epoch: [2][440/880] Elapsed 17m 39s (remain 17m 34s) Loss: 0.9601(1.2561) Grad: 216657.5312  LR: 0.00001382  \n",
      "Epoch: [2][460/880] Elapsed 18m 27s (remain 16m 46s) Loss: 1.1492(1.2574) Grad: 182070.4062  LR: 0.00001365  \n",
      "Epoch: [2][480/880] Elapsed 19m 12s (remain 15m 56s) Loss: 1.6748(1.2600) Grad: 283194.0625  LR: 0.00001349  \n",
      "Epoch: [2][500/880] Elapsed 20m 6s (remain 15m 12s) Loss: 1.2323(1.2609) Grad: 242225.0156  LR: 0.00001332  \n",
      "Epoch: [2][520/880] Elapsed 20m 46s (remain 14m 18s) Loss: 1.2681(1.2597) Grad: 253665.4531  LR: 0.00001315  \n",
      "Epoch: [2][540/880] Elapsed 21m 39s (remain 13m 33s) Loss: 1.5247(1.2595) Grad: 255338.7812  LR: 0.00001298  \n",
      "Epoch: [2][560/880] Elapsed 22m 28s (remain 12m 46s) Loss: 1.2999(1.2614) Grad: 354203.7500  LR: 0.00001281  \n",
      "Epoch: [2][580/880] Elapsed 23m 12s (remain 11m 56s) Loss: 1.2516(1.2616) Grad: 277258.7500  LR: 0.00001264  \n",
      "Epoch: [2][600/880] Elapsed 24m 0s (remain 11m 8s) Loss: 1.1972(1.2608) Grad: 236593.7969  LR: 0.00001246  \n",
      "Epoch: [2][620/880] Elapsed 24m 48s (remain 10m 20s) Loss: 1.9832(1.2608) Grad: 390793.3750  LR: 0.00001229  \n",
      "Epoch: [2][640/880] Elapsed 25m 30s (remain 9m 30s) Loss: 1.4126(1.2589) Grad: 250242.0781  LR: 0.00001212  \n",
      "Epoch: [2][660/880] Elapsed 26m 15s (remain 8m 42s) Loss: 1.2890(1.2585) Grad: 270585.2188  LR: 0.00001194  \n",
      "Epoch: [2][680/880] Elapsed 26m 59s (remain 7m 53s) Loss: 1.2231(1.2583) Grad: 252730.2656  LR: 0.00001177  \n",
      "Epoch: [2][700/880] Elapsed 27m 49s (remain 7m 6s) Loss: 1.1271(1.2587) Grad: 302541.7500  LR: 0.00001159  \n",
      "Epoch: [2][720/880] Elapsed 28m 37s (remain 6m 18s) Loss: 1.1336(1.2578) Grad: 183874.4219  LR: 0.00001141  \n",
      "Epoch: [2][740/880] Elapsed 29m 23s (remain 5m 30s) Loss: 1.0896(1.2573) Grad: 217881.9844  LR: 0.00001124  \n",
      "Epoch: [2][760/880] Elapsed 30m 5s (remain 4m 42s) Loss: 1.6281(1.2552) Grad: 265285.7188  LR: 0.00001106  \n",
      "Epoch: [2][780/880] Elapsed 30m 58s (remain 3m 55s) Loss: 1.1653(1.2536) Grad: 254859.7500  LR: 0.00001088  \n",
      "Epoch: [2][800/880] Elapsed 31m 44s (remain 3m 7s) Loss: 1.3468(1.2530) Grad: 261560.8906  LR: 0.00001070  \n",
      "Epoch: [2][820/880] Elapsed 32m 34s (remain 2m 20s) Loss: 1.4992(1.2538) Grad: 260116.2031  LR: 0.00001053  \n",
      "Epoch: [2][840/880] Elapsed 33m 14s (remain 1m 32s) Loss: 1.4214(1.2527) Grad: 270244.5312  LR: 0.00001035  \n",
      "Epoch: [2][860/880] Elapsed 34m 2s (remain 0m 45s) Loss: 1.3417(1.2520) Grad: 274415.6250  LR: 0.00001017  \n",
      "Epoch: [2][879/880] Elapsed 34m 45s (remain 0m 0s) Loss: 1.1544(1.2507) Grad: 224640.6406  LR: 0.00001000  \n",
      "EVAL: [0/49] Elapsed 0m 3s (remain 2m 48s) Loss: 1.2380(1.2380) \n",
      "EVAL: [20/49] Elapsed 0m 35s (remain 0m 47s) Loss: 1.2060(1.3562) \n",
      "EVAL: [40/49] Elapsed 1m 0s (remain 0m 11s) Loss: 1.5134(1.3736) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 1.2507  avg_val_loss: 1.3708  time: 2159s\n",
      "Epoch 2 - Score: 0.5036  Scores: [0.5231226328525249, 0.48176465904087157, 0.4993602044724246, 0.5113794593124069, 0.506984465325952, 0.49871958816702133]\n",
      "Epoch 2 - Save Best Score: 0.5036 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [48/49] Elapsed 1m 13s (remain 0m 0s) Loss: 1.0768(1.3708) \n",
      "Epoch: [3][0/880] Elapsed 0m 2s (remain 32m 53s) Loss: 1.2064(1.2064) Grad: 434334.6562  LR: 0.00000999  \n",
      "Epoch: [3][20/880] Elapsed 0m 46s (remain 31m 25s) Loss: 1.0815(1.1575) Grad: 439819.8750  LR: 0.00000981  \n",
      "Epoch: [3][40/880] Elapsed 1m 25s (remain 29m 17s) Loss: 1.1295(1.1395) Grad: 500323.6562  LR: 0.00000963  \n",
      "Epoch: [3][60/880] Elapsed 2m 15s (remain 30m 22s) Loss: 0.9930(1.1226) Grad: 402704.3750  LR: 0.00000946  \n",
      "Epoch: [3][80/880] Elapsed 3m 1s (remain 29m 54s) Loss: 0.9714(1.1427) Grad: 369382.5000  LR: 0.00000928  \n",
      "Epoch: [3][100/880] Elapsed 3m 43s (remain 28m 43s) Loss: 1.1596(1.1417) Grad: 487998.4375  LR: 0.00000910  \n",
      "Epoch: [3][120/880] Elapsed 4m 33s (remain 28m 33s) Loss: 1.0179(1.1415) Grad: 454749.2500  LR: 0.00000892  \n",
      "Epoch: [3][140/880] Elapsed 5m 20s (remain 28m 2s) Loss: 1.1969(1.1365) Grad: 612790.9375  LR: 0.00000874  \n",
      "Epoch: [3][160/880] Elapsed 6m 16s (remain 28m 2s) Loss: 1.2213(1.1344) Grad: 462786.7500  LR: 0.00000857  \n",
      "Epoch: [3][180/880] Elapsed 7m 8s (remain 27m 34s) Loss: 1.0825(1.1274) Grad: 475877.0938  LR: 0.00000839  \n",
      "Epoch: [3][200/880] Elapsed 7m 49s (remain 26m 25s) Loss: 0.9542(1.1234) Grad: 449868.0000  LR: 0.00000822  \n",
      "Epoch: [3][220/880] Elapsed 8m 33s (remain 25m 32s) Loss: 1.3014(1.1243) Grad: 487013.8125  LR: 0.00000804  \n",
      "Epoch: [3][240/880] Elapsed 9m 14s (remain 24m 30s) Loss: 0.9702(1.1206) Grad: 335237.2500  LR: 0.00000787  \n",
      "Epoch: [3][260/880] Elapsed 9m 57s (remain 23m 36s) Loss: 0.9979(1.1178) Grad: 407835.5938  LR: 0.00000769  \n",
      "Epoch: [3][280/880] Elapsed 10m 48s (remain 23m 1s) Loss: 1.1003(1.1162) Grad: 357659.5938  LR: 0.00000752  \n",
      "Epoch: [3][300/880] Elapsed 11m 35s (remain 22m 17s) Loss: 1.4541(1.1182) Grad: 625560.9375  LR: 0.00000735  \n",
      "Epoch: [3][320/880] Elapsed 12m 26s (remain 21m 40s) Loss: 1.2384(1.1195) Grad: 495030.5938  LR: 0.00000717  \n",
      "Epoch: [3][340/880] Elapsed 13m 10s (remain 20m 49s) Loss: 1.1892(1.1208) Grad: 553235.8125  LR: 0.00000700  \n",
      "Epoch: [3][360/880] Elapsed 14m 1s (remain 20m 9s) Loss: 1.0704(1.1228) Grad: 465027.7812  LR: 0.00000683  \n",
      "Epoch: [3][380/880] Elapsed 14m 58s (remain 19m 36s) Loss: 1.1325(1.1252) Grad: 558376.8125  LR: 0.00000666  \n",
      "Epoch: [3][400/880] Elapsed 15m 46s (remain 18m 50s) Loss: 1.0400(1.1238) Grad: 511961.5625  LR: 0.00000650  \n",
      "Epoch: [3][420/880] Elapsed 16m 35s (remain 18m 5s) Loss: 1.1374(1.1224) Grad: 506455.7812  LR: 0.00000633  \n",
      "Epoch: [3][440/880] Elapsed 17m 23s (remain 17m 18s) Loss: 0.9780(1.1208) Grad: 451458.9062  LR: 0.00000616  \n",
      "Epoch: [3][460/880] Elapsed 18m 13s (remain 16m 34s) Loss: 1.1615(1.1190) Grad: 445193.9688  LR: 0.00000600  \n",
      "Epoch: [3][480/880] Elapsed 18m 58s (remain 15m 44s) Loss: 1.2750(1.1183) Grad: 691103.0625  LR: 0.00000584  \n",
      "Epoch: [3][500/880] Elapsed 19m 51s (remain 15m 1s) Loss: 1.0710(1.1200) Grad: 839497.2500  LR: 0.00000568  \n",
      "Epoch: [3][520/880] Elapsed 20m 42s (remain 14m 15s) Loss: 1.1543(1.1208) Grad: 385588.2188  LR: 0.00000552  \n",
      "Epoch: [3][540/880] Elapsed 21m 27s (remain 13m 26s) Loss: 1.0687(1.1213) Grad: 445028.4062  LR: 0.00000536  \n",
      "Epoch: [3][560/880] Elapsed 22m 24s (remain 12m 44s) Loss: 1.2176(1.1189) Grad: 764757.7500  LR: 0.00000520  \n",
      "Epoch: [3][580/880] Elapsed 23m 7s (remain 11m 54s) Loss: 1.0088(1.1172) Grad: 476310.7812  LR: 0.00000504  \n",
      "Epoch: [3][600/880] Elapsed 24m 10s (remain 11m 13s) Loss: 0.9319(1.1171) Grad: 350190.2188  LR: 0.00000489  \n",
      "Epoch: [3][620/880] Elapsed 24m 52s (remain 10m 22s) Loss: 1.2065(1.1184) Grad: 1051163.8750  LR: 0.00000474  \n",
      "Epoch: [3][640/880] Elapsed 25m 34s (remain 9m 32s) Loss: 1.3517(1.1179) Grad: 476931.6562  LR: 0.00000459  \n",
      "Epoch: [3][660/880] Elapsed 26m 17s (remain 8m 42s) Loss: 0.9973(1.1193) Grad: 379793.3125  LR: 0.00000444  \n",
      "Epoch: [3][680/880] Elapsed 27m 8s (remain 7m 55s) Loss: 1.2022(1.1207) Grad: 613937.6250  LR: 0.00000429  \n",
      "Epoch: [3][700/880] Elapsed 27m 54s (remain 7m 7s) Loss: 0.9864(1.1208) Grad: 479563.7188  LR: 0.00000414  \n",
      "Epoch: [3][720/880] Elapsed 28m 41s (remain 6m 19s) Loss: 1.0590(1.1199) Grad: 549312.8750  LR: 0.00000400  \n",
      "Epoch: [3][740/880] Elapsed 29m 30s (remain 5m 32s) Loss: 1.1023(1.1191) Grad: 515392.8438  LR: 0.00000386  \n",
      "Epoch: [3][760/880] Elapsed 30m 10s (remain 4m 43s) Loss: 0.9606(1.1177) Grad: 534428.1250  LR: 0.00000372  \n",
      "Epoch: [3][780/880] Elapsed 30m 57s (remain 3m 55s) Loss: 1.0461(1.1174) Grad: 601858.9375  LR: 0.00000358  \n",
      "Epoch: [3][800/880] Elapsed 31m 44s (remain 3m 7s) Loss: 0.9835(1.1153) Grad: 373604.3125  LR: 0.00000344  \n",
      "Epoch: [3][820/880] Elapsed 32m 32s (remain 2m 20s) Loss: 1.6352(1.1157) Grad: 696374.2500  LR: 0.00000331  \n",
      "Epoch: [3][840/880] Elapsed 33m 26s (remain 1m 33s) Loss: 0.9957(1.1148) Grad: 557813.3125  LR: 0.00000318  \n",
      "Epoch: [3][860/880] Elapsed 34m 13s (remain 0m 45s) Loss: 1.3102(1.1144) Grad: 736438.5000  LR: 0.00000305  \n",
      "Epoch: [3][879/880] Elapsed 34m 57s (remain 0m 0s) Loss: 1.0395(1.1143) Grad: 469372.3125  LR: 0.00000293  \n",
      "EVAL: [0/49] Elapsed 0m 3s (remain 2m 48s) Loss: 1.1739(1.1739) \n",
      "EVAL: [20/49] Elapsed 0m 35s (remain 0m 47s) Loss: 1.2355(1.3660) \n",
      "EVAL: [40/49] Elapsed 1m 0s (remain 0m 11s) Loss: 1.5272(1.3997) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 1.1143  avg_val_loss: 1.3948  time: 2171s\n",
      "Epoch 3 - Score: 0.5085  Scores: [0.5358025746270562, 0.48507125007266594, 0.47776654295295456, 0.5273831551418434, 0.5194429438032705, 0.5057217374241736]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [48/49] Elapsed 1m 13s (remain 0m 0s) Loss: 1.0830(1.3948) \n",
      "Epoch: [4][0/880] Elapsed 0m 2s (remain 33m 24s) Loss: 0.8143(0.8143) Grad: 381840.8750  LR: 0.00000292  \n",
      "Epoch: [4][20/880] Elapsed 0m 48s (remain 33m 21s) Loss: 1.0018(1.0558) Grad: 404046.0625  LR: 0.00000280  \n",
      "Epoch: [4][40/880] Elapsed 1m 33s (remain 31m 59s) Loss: 1.0649(1.0577) Grad: 375336.0000  LR: 0.00000267  \n",
      "Epoch: [4][60/880] Elapsed 2m 19s (remain 31m 13s) Loss: 0.8655(1.0592) Grad: 449598.5000  LR: 0.00000255  \n",
      "Epoch: [4][80/880] Elapsed 3m 0s (remain 29m 41s) Loss: 1.0056(1.0674) Grad: 521294.1875  LR: 0.00000244  \n",
      "Epoch: [4][100/880] Elapsed 3m 48s (remain 29m 18s) Loss: 0.9467(1.0652) Grad: 435921.4062  LR: 0.00000232  \n",
      "Epoch: [4][120/880] Elapsed 4m 28s (remain 28m 1s) Loss: 1.0262(1.0578) Grad: 378585.0000  LR: 0.00000221  \n",
      "Epoch: [4][140/880] Elapsed 5m 11s (remain 27m 11s) Loss: 1.1349(1.0501) Grad: 474161.4375  LR: 0.00000210  \n",
      "Epoch: [4][160/880] Elapsed 6m 2s (remain 26m 57s) Loss: 1.1878(1.0478) Grad: 544789.0000  LR: 0.00000199  \n",
      "Epoch: [4][180/880] Elapsed 6m 59s (remain 27m 0s) Loss: 1.0251(1.0460) Grad: 355145.5312  LR: 0.00000188  \n",
      "Epoch: [4][200/880] Elapsed 7m 44s (remain 26m 10s) Loss: 1.5317(1.0503) Grad: 797473.2500  LR: 0.00000178  \n",
      "Epoch: [4][220/880] Elapsed 8m 35s (remain 25m 36s) Loss: 1.0061(1.0498) Grad: 695819.8750  LR: 0.00000168  \n",
      "Epoch: [4][240/880] Elapsed 9m 20s (remain 24m 45s) Loss: 1.1031(1.0482) Grad: 611087.6875  LR: 0.00000158  \n",
      "Epoch: [4][260/880] Elapsed 10m 10s (remain 24m 7s) Loss: 1.2173(1.0472) Grad: 629085.6250  LR: 0.00000149  \n",
      "Epoch: [4][280/880] Elapsed 11m 7s (remain 23m 43s) Loss: 1.4128(1.0483) Grad: 554297.0625  LR: 0.00000140  \n",
      "Epoch: [4][300/880] Elapsed 11m 56s (remain 22m 58s) Loss: 0.9209(1.0454) Grad: 410533.3750  LR: 0.00000131  \n",
      "Epoch: [4][320/880] Elapsed 12m 37s (remain 21m 59s) Loss: 1.0010(1.0445) Grad: 494165.3750  LR: 0.00000122  \n",
      "Epoch: [4][340/880] Elapsed 13m 28s (remain 21m 17s) Loss: 1.3608(1.0445) Grad: 768657.4375  LR: 0.00000113  \n",
      "Epoch: [4][360/880] Elapsed 14m 14s (remain 20m 27s) Loss: 0.8909(1.0422) Grad: 605591.0000  LR: 0.00000105  \n",
      "Epoch: [4][380/880] Elapsed 15m 1s (remain 19m 41s) Loss: 1.0283(1.0410) Grad: 668444.7500  LR: 0.00000098  \n",
      "Epoch: [4][400/880] Elapsed 15m 54s (remain 19m 0s) Loss: 1.1154(1.0440) Grad: 746226.1875  LR: 0.00000090  \n",
      "Epoch: [4][420/880] Elapsed 16m 47s (remain 18m 18s) Loss: 0.9853(1.0462) Grad: 478407.4062  LR: 0.00000083  \n",
      "Epoch: [4][440/880] Elapsed 17m 35s (remain 17m 30s) Loss: 1.0352(1.0474) Grad: 436364.6875  LR: 0.00000076  \n",
      "Epoch: [4][460/880] Elapsed 18m 26s (remain 16m 45s) Loss: 1.0296(1.0452) Grad: 572318.5625  LR: 0.00000069  \n",
      "Epoch: [4][480/880] Elapsed 19m 17s (remain 16m 0s) Loss: 1.0148(1.0439) Grad: 367548.9062  LR: 0.00000063  \n",
      "Epoch: [4][500/880] Elapsed 20m 5s (remain 15m 11s) Loss: 0.8484(1.0455) Grad: 379263.8125  LR: 0.00000057  \n",
      "Epoch: [4][520/880] Elapsed 20m 49s (remain 14m 20s) Loss: 1.0036(1.0429) Grad: 453063.8750  LR: 0.00000051  \n",
      "Epoch: [4][540/880] Elapsed 21m 30s (remain 13m 28s) Loss: 0.9334(1.0442) Grad: 376952.0312  LR: 0.00000045  \n",
      "Epoch: [4][560/880] Elapsed 22m 17s (remain 12m 40s) Loss: 0.9897(1.0443) Grad: 662926.1250  LR: 0.00000040  \n",
      "Epoch: [4][580/880] Elapsed 23m 8s (remain 11m 54s) Loss: 1.0799(1.0437) Grad: 498251.8750  LR: 0.00000035  \n",
      "Epoch: [4][600/880] Elapsed 23m 55s (remain 11m 6s) Loss: 0.9076(1.0447) Grad: 544634.0625  LR: 0.00000031  \n",
      "Epoch: [4][620/880] Elapsed 24m 44s (remain 10m 18s) Loss: 0.9300(1.0458) Grad: 425215.7500  LR: 0.00000027  \n",
      "Epoch: [4][640/880] Elapsed 25m 25s (remain 9m 28s) Loss: 0.9043(1.0462) Grad: 364502.4688  LR: 0.00000023  \n",
      "Epoch: [4][660/880] Elapsed 26m 15s (remain 8m 42s) Loss: 0.9172(1.0463) Grad: 456252.2500  LR: 0.00000019  \n",
      "Epoch: [4][680/880] Elapsed 26m 57s (remain 7m 52s) Loss: 1.0691(1.0458) Grad: 390927.2188  LR: 0.00000016  \n",
      "Epoch: [4][700/880] Elapsed 27m 42s (remain 7m 4s) Loss: 1.0298(1.0452) Grad: 485117.5000  LR: 0.00000013  \n",
      "Epoch: [4][720/880] Elapsed 28m 30s (remain 6m 17s) Loss: 1.3195(1.0469) Grad: 502966.1562  LR: 0.00000010  \n",
      "Epoch: [4][740/880] Elapsed 29m 21s (remain 5m 30s) Loss: 1.0363(1.0464) Grad: 716007.2500  LR: 0.00000008  \n",
      "Epoch: [4][760/880] Elapsed 30m 13s (remain 4m 43s) Loss: 0.8805(1.0465) Grad: 787356.5625  LR: 0.00000006  \n",
      "Epoch: [4][780/880] Elapsed 31m 4s (remain 3m 56s) Loss: 1.1345(1.0462) Grad: 473222.4375  LR: 0.00000004  \n",
      "Epoch: [4][800/880] Elapsed 31m 51s (remain 3m 8s) Loss: 1.1540(1.0469) Grad: 767135.8125  LR: 0.00000002  \n",
      "Epoch: [4][820/880] Elapsed 32m 44s (remain 2m 21s) Loss: 1.0068(1.0456) Grad: 501547.7812  LR: 0.00000001  \n",
      "Epoch: [4][840/880] Elapsed 33m 28s (remain 1m 33s) Loss: 1.2054(1.0445) Grad: 508086.8438  LR: 0.00000001  \n",
      "Epoch: [4][860/880] Elapsed 34m 17s (remain 0m 45s) Loss: 1.0497(1.0442) Grad: 484617.3125  LR: 0.00000000  \n",
      "Epoch: [4][879/880] Elapsed 35m 0s (remain 0m 0s) Loss: 1.2441(1.0433) Grad: 584773.3750  LR: 0.00000000  \n",
      "EVAL: [0/49] Elapsed 0m 3s (remain 2m 48s) Loss: 1.2224(1.2224) \n",
      "EVAL: [20/49] Elapsed 0m 35s (remain 0m 47s) Loss: 1.2495(1.3723) \n",
      "EVAL: [40/49] Elapsed 1m 0s (remain 0m 11s) Loss: 1.5691(1.4016) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 1.0433  avg_val_loss: 1.3942  time: 2175s\n",
      "Epoch 4 - Score: 0.5064  Scores: [0.5358025746270562, 0.4877003817961311, 0.47776654295295456, 0.5231226328525249, 0.506984465325952, 0.506984465325952]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [48/49] Elapsed 1m 13s (remain 0m 0s) Loss: 1.0351(1.3942) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 6 result ==========\n",
      "Score: 0.5036  Scores: [0.5231226328525249, 0.48176465904087157, 0.4993602044724246, 0.5113794593124069, 0.506984465325952, 0.49871958816702133]\n",
      "========== fold: 7 training ==========\n",
      "DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-large\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 1024,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/880] Elapsed 0m 1s (remain 27m 31s) Loss: 2.2548(2.2548) Grad: inf  LR: 0.00002000  \n",
      "Epoch: [1][20/880] Elapsed 0m 55s (remain 37m 38s) Loss: 1.8821(1.9693) Grad: 113506.8828  LR: 0.00002000  \n",
      "Epoch: [1][40/880] Elapsed 1m 41s (remain 34m 41s) Loss: 1.3934(1.8338) Grad: 140276.2031  LR: 0.00001999  \n",
      "Epoch: [1][60/880] Elapsed 2m 26s (remain 32m 51s) Loss: 1.4784(1.7744) Grad: 150371.7656  LR: 0.00001999  \n",
      "Epoch: [1][80/880] Elapsed 3m 14s (remain 32m 0s) Loss: 1.5229(1.7322) Grad: 116519.5469  LR: 0.00001997  \n",
      "Epoch: [1][100/880] Elapsed 4m 4s (remain 31m 29s) Loss: 1.8180(1.7179) Grad: 167477.6562  LR: 0.00001996  \n",
      "Epoch: [1][120/880] Elapsed 4m 52s (remain 30m 37s) Loss: 1.2023(1.6757) Grad: 80562.0625  LR: 0.00001994  \n",
      "Epoch: [1][140/880] Elapsed 5m 40s (remain 29m 44s) Loss: 1.6025(1.6587) Grad: 217416.8594  LR: 0.00001992  \n",
      "Epoch: [1][160/880] Elapsed 6m 29s (remain 28m 58s) Loss: 1.2735(1.6423) Grad: 104496.2734  LR: 0.00001990  \n",
      "Epoch: [1][180/880] Elapsed 7m 26s (remain 28m 44s) Loss: 1.5245(1.6354) Grad: 168937.1875  LR: 0.00001987  \n",
      "Epoch: [1][200/880] Elapsed 8m 26s (remain 28m 32s) Loss: 1.4940(1.6164) Grad: 128853.8984  LR: 0.00001984  \n",
      "Epoch: [1][220/880] Elapsed 9m 10s (remain 27m 21s) Loss: 1.4660(1.6051) Grad: 122127.0859  LR: 0.00001981  \n",
      "Epoch: [1][240/880] Elapsed 9m 55s (remain 26m 20s) Loss: 1.3105(1.5960) Grad: 113259.1562  LR: 0.00001977  \n",
      "Epoch: [1][260/880] Elapsed 10m 53s (remain 25m 49s) Loss: 1.2172(1.5897) Grad: 122029.9844  LR: 0.00001973  \n",
      "Epoch: [1][280/880] Elapsed 11m 38s (remain 24m 49s) Loss: 1.1661(1.5789) Grad: 91296.3047  LR: 0.00001969  \n",
      "Epoch: [1][300/880] Elapsed 12m 23s (remain 23m 50s) Loss: 1.5393(1.5657) Grad: 177099.9375  LR: 0.00001964  \n",
      "Epoch: [1][320/880] Elapsed 13m 7s (remain 22m 51s) Loss: 1.2421(1.5568) Grad: 128359.7344  LR: 0.00001959  \n",
      "Epoch: [1][340/880] Elapsed 13m 52s (remain 21m 55s) Loss: 1.3600(1.5490) Grad: 104629.0781  LR: 0.00001954  \n",
      "Epoch: [1][360/880] Elapsed 14m 38s (remain 21m 3s) Loss: 1.4181(1.5413) Grad: 153088.3281  LR: 0.00001949  \n",
      "Epoch: [1][380/880] Elapsed 15m 22s (remain 20m 8s) Loss: 1.5847(1.5323) Grad: 228346.6562  LR: 0.00001943  \n",
      "Epoch: [1][400/880] Elapsed 16m 8s (remain 19m 17s) Loss: 1.8089(1.5308) Grad: 232567.6562  LR: 0.00001937  \n",
      "Epoch: [1][420/880] Elapsed 16m 58s (remain 18m 30s) Loss: 1.7639(1.5283) Grad: 220622.0781  LR: 0.00001930  \n",
      "Epoch: [1][440/880] Elapsed 17m 43s (remain 17m 38s) Loss: 1.9310(1.5238) Grad: 119273.2344  LR: 0.00001924  \n",
      "Epoch: [1][460/880] Elapsed 18m 26s (remain 16m 45s) Loss: 1.3726(1.5199) Grad: 98405.0547  LR: 0.00001917  \n",
      "Epoch: [1][480/880] Elapsed 19m 16s (remain 15m 59s) Loss: 1.2462(1.5144) Grad: 106470.2891  LR: 0.00001909  \n",
      "Epoch: [1][500/880] Elapsed 20m 7s (remain 15m 13s) Loss: 1.4469(1.5134) Grad: 123090.7656  LR: 0.00001902  \n",
      "Epoch: [1][520/880] Elapsed 20m 54s (remain 14m 24s) Loss: 1.5268(1.5090) Grad: 108796.2266  LR: 0.00001894  \n",
      "Epoch: [1][540/880] Elapsed 21m 39s (remain 13m 34s) Loss: 1.2261(1.5066) Grad: 91775.5781  LR: 0.00001886  \n",
      "Epoch: [1][560/880] Elapsed 22m 31s (remain 12m 48s) Loss: 1.6605(1.5046) Grad: 149365.0156  LR: 0.00001877  \n",
      "Epoch: [1][580/880] Elapsed 23m 12s (remain 11m 56s) Loss: 1.4987(1.5005) Grad: 90348.6406  LR: 0.00001869  \n",
      "Epoch: [1][600/880] Elapsed 24m 8s (remain 11m 12s) Loss: 1.3856(1.4968) Grad: 154587.5312  LR: 0.00001860  \n",
      "Epoch: [1][620/880] Elapsed 24m 59s (remain 10m 25s) Loss: 1.1267(1.4928) Grad: 76889.2812  LR: 0.00001850  \n",
      "Epoch: [1][640/880] Elapsed 25m 50s (remain 9m 38s) Loss: 1.3829(1.4906) Grad: 112998.5000  LR: 0.00001841  \n",
      "Epoch: [1][660/880] Elapsed 26m 37s (remain 8m 49s) Loss: 1.1535(1.4854) Grad: 99913.4219  LR: 0.00001831  \n",
      "Epoch: [1][740/880] Elapsed 29m 47s (remain 5m 35s) Loss: 1.3039(1.4742) Grad: 110162.3750  LR: 0.00001789  \n",
      "Epoch: [1][760/880] Elapsed 30m 34s (remain 4m 46s) Loss: 1.2869(1.4714) Grad: 105011.8281  LR: 0.00001778  \n",
      "Epoch: [1][780/880] Elapsed 31m 21s (remain 3m 58s) Loss: 1.4532(1.4697) Grad: 177682.6719  LR: 0.00001767  \n",
      "Epoch: [1][800/880] Elapsed 32m 9s (remain 3m 10s) Loss: 1.5176(1.4684) Grad: 127859.5469  LR: 0.00001755  \n",
      "Epoch: [1][820/880] Elapsed 32m 57s (remain 2m 22s) Loss: 1.3959(1.4682) Grad: 98333.6953  LR: 0.00001743  \n",
      "Epoch: [1][840/880] Elapsed 33m 38s (remain 1m 33s) Loss: 2.0425(1.4661) Grad: 135223.1875  LR: 0.00001731  \n",
      "Epoch: [1][860/880] Elapsed 34m 24s (remain 0m 45s) Loss: 1.5034(1.4639) Grad: 105395.6094  LR: 0.00001719  \n",
      "Epoch: [1][879/880] Elapsed 35m 5s (remain 0m 0s) Loss: 1.1518(1.4613) Grad: 81724.7344  LR: 0.00001707  \n",
      "EVAL: [0/49] Elapsed 0m 1s (remain 1m 26s) Loss: 1.4044(1.4044) \n",
      "EVAL: [20/49] Elapsed 0m 33s (remain 0m 44s) Loss: 1.4086(1.3679) \n",
      "EVAL: [40/49] Elapsed 1m 1s (remain 0m 12s) Loss: 1.5340(1.3613) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 1.4613  avg_val_loss: 1.3507  time: 2178s\n",
      "Epoch 1 - Score: 0.5035  Scores: [0.5405547969364888, 0.5031867754087855, 0.4837513258571414, 0.5144957554275266, 0.48242779033760647, 0.496792782811615]\n",
      "Epoch 1 - Save Best Score: 0.5035 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [48/49] Elapsed 1m 12s (remain 0m 0s) Loss: 1.2281(1.3507) \n",
      "Epoch: [2][0/880] Elapsed 0m 4s (remain 67m 38s) Loss: 1.1944(1.1944) Grad: 384504.0625  LR: 0.00001706  \n",
      "Epoch: [2][20/880] Elapsed 0m 49s (remain 33m 31s) Loss: 1.6016(1.3260) Grad: 286855.6562  LR: 0.00001694  \n",
      "Epoch: [2][40/880] Elapsed 1m 39s (remain 34m 2s) Loss: 1.1992(1.3305) Grad: 268427.8438  LR: 0.00001681  \n",
      "Epoch: [2][60/880] Elapsed 2m 25s (remain 32m 29s) Loss: 1.3475(1.3207) Grad: 204196.0000  LR: 0.00001668  \n",
      "Epoch: [2][80/880] Elapsed 3m 21s (remain 33m 3s) Loss: 1.5837(1.3080) Grad: 571009.6250  LR: 0.00001654  \n",
      "Epoch: [2][100/880] Elapsed 4m 5s (remain 31m 35s) Loss: 1.1847(1.2967) Grad: 198410.1406  LR: 0.00001641  \n",
      "Epoch: [2][120/880] Elapsed 4m 48s (remain 30m 11s) Loss: 1.1997(1.2842) Grad: 181210.8125  LR: 0.00001627  \n",
      "Epoch: [2][140/880] Elapsed 5m 38s (remain 29m 36s) Loss: 1.1527(1.2770) Grad: 215679.8438  LR: 0.00001613  \n",
      "Epoch: [2][160/880] Elapsed 6m 23s (remain 28m 32s) Loss: 1.5699(1.2776) Grad: 336595.3125  LR: 0.00001599  \n",
      "Epoch: [2][180/880] Elapsed 7m 12s (remain 27m 51s) Loss: 1.2396(1.2847) Grad: 205173.4531  LR: 0.00001584  \n",
      "Epoch: [2][200/880] Elapsed 7m 56s (remain 26m 50s) Loss: 1.1559(1.2764) Grad: 190516.8281  LR: 0.00001570  \n",
      "Epoch: [2][220/880] Elapsed 8m 44s (remain 26m 4s) Loss: 1.1481(1.2755) Grad: 216911.4375  LR: 0.00001555  \n",
      "Epoch: [2][240/880] Elapsed 9m 27s (remain 25m 5s) Loss: 1.5300(1.2724) Grad: 299764.7500  LR: 0.00001540  \n",
      "Epoch: [2][260/880] Elapsed 10m 17s (remain 24m 23s) Loss: 1.3383(1.2695) Grad: 221596.7188  LR: 0.00001525  \n",
      "Epoch: [2][280/880] Elapsed 11m 13s (remain 23m 56s) Loss: 0.9592(1.2651) Grad: 302834.2188  LR: 0.00001510  \n",
      "Epoch: [2][300/880] Elapsed 12m 2s (remain 23m 10s) Loss: 1.0625(1.2656) Grad: 227933.0156  LR: 0.00001494  \n",
      "Epoch: [2][320/880] Elapsed 12m 53s (remain 22m 26s) Loss: 1.2676(1.2679) Grad: 203413.2031  LR: 0.00001478  \n",
      "Epoch: [2][340/880] Elapsed 13m 42s (remain 21m 40s) Loss: 1.1771(1.2698) Grad: 205019.3750  LR: 0.00001463  \n",
      "Epoch: [2][360/880] Elapsed 14m 27s (remain 20m 47s) Loss: 1.0675(1.2726) Grad: 168492.8438  LR: 0.00001447  \n",
      "Epoch: [2][380/880] Elapsed 15m 21s (remain 20m 6s) Loss: 1.1382(1.2735) Grad: 296477.8125  LR: 0.00001431  \n",
      "Epoch: [2][400/880] Elapsed 16m 7s (remain 19m 15s) Loss: 1.3974(1.2748) Grad: 248309.1875  LR: 0.00001415  \n",
      "Epoch: [2][420/880] Elapsed 16m 52s (remain 18m 24s) Loss: 1.2645(1.2772) Grad: 220918.7812  LR: 0.00001398  \n",
      "Epoch: [2][440/880] Elapsed 17m 37s (remain 17m 32s) Loss: 1.2486(1.2738) Grad: 216003.6719  LR: 0.00001382  \n",
      "Epoch: [2][460/880] Elapsed 18m 24s (remain 16m 43s) Loss: 1.2366(1.2728) Grad: 264590.4688  LR: 0.00001365  \n",
      "Epoch: [2][480/880] Elapsed 19m 19s (remain 16m 1s) Loss: 1.7041(1.2725) Grad: 471031.6875  LR: 0.00001349  \n",
      "Epoch: [2][500/880] Elapsed 20m 8s (remain 15m 14s) Loss: 1.1852(1.2739) Grad: 219665.1875  LR: 0.00001332  \n",
      "Epoch: [2][520/880] Elapsed 20m 50s (remain 14m 21s) Loss: 1.3699(1.2732) Grad: 280408.7500  LR: 0.00001315  \n",
      "Epoch: [2][540/880] Elapsed 21m 38s (remain 13m 33s) Loss: 1.4696(1.2704) Grad: nan  LR: 0.00001298  \n",
      "Epoch: [2][560/880] Elapsed 22m 25s (remain 12m 44s) Loss: 1.2091(1.2701) Grad: 160314.2188  LR: 0.00001281  \n",
      "Epoch: [2][580/880] Elapsed 23m 12s (remain 11m 56s) Loss: 1.2401(1.2727) Grad: 171380.0156  LR: 0.00001264  \n",
      "Epoch: [2][600/880] Elapsed 24m 4s (remain 11m 10s) Loss: 1.0351(1.2728) Grad: 82210.5469  LR: 0.00001246  \n",
      "Epoch: [2][620/880] Elapsed 24m 54s (remain 10m 23s) Loss: 1.3147(1.2747) Grad: 101432.4922  LR: 0.00001229  \n",
      "Epoch: [2][640/880] Elapsed 25m 42s (remain 9m 34s) Loss: 1.0798(1.2741) Grad: 85885.1172  LR: 0.00001212  \n",
      "Epoch: [2][660/880] Elapsed 26m 27s (remain 8m 46s) Loss: 1.1684(1.2730) Grad: 124446.1641  LR: 0.00001194  \n",
      "Epoch: [2][680/880] Elapsed 27m 18s (remain 7m 58s) Loss: 0.9938(1.2723) Grad: 108302.1328  LR: 0.00001177  \n",
      "Epoch: [2][700/880] Elapsed 27m 58s (remain 7m 8s) Loss: 1.0197(1.2703) Grad: 90142.4688  LR: 0.00001159  \n",
      "Epoch: [2][720/880] Elapsed 28m 43s (remain 6m 19s) Loss: 1.1136(1.2713) Grad: 97885.3984  LR: 0.00001141  \n",
      "Epoch: [2][740/880] Elapsed 29m 29s (remain 5m 31s) Loss: 1.1689(1.2715) Grad: 99451.5000  LR: 0.00001124  \n",
      "Epoch: [2][760/880] Elapsed 30m 12s (remain 4m 43s) Loss: 1.3750(1.2716) Grad: 123756.3203  LR: 0.00001106  \n",
      "Epoch: [2][780/880] Elapsed 31m 2s (remain 3m 56s) Loss: 1.3209(1.2700) Grad: 134563.7031  LR: 0.00001088  \n",
      "Epoch: [2][800/880] Elapsed 31m 56s (remain 3m 9s) Loss: 0.9726(1.2692) Grad: 89136.6250  LR: 0.00001070  \n",
      "Epoch: [2][820/880] Elapsed 32m 40s (remain 2m 20s) Loss: 1.1615(1.2677) Grad: 92855.6172  LR: 0.00001053  \n",
      "Epoch: [2][840/880] Elapsed 33m 29s (remain 1m 33s) Loss: 1.2530(1.2680) Grad: 124785.4531  LR: 0.00001035  \n",
      "Epoch: [2][860/880] Elapsed 34m 19s (remain 0m 45s) Loss: 1.2106(1.2675) Grad: 131691.2500  LR: 0.00001017  \n",
      "Epoch: [2][879/880] Elapsed 35m 3s (remain 0m 0s) Loss: 1.5198(1.2665) Grad: 142888.6719  LR: 0.00001000  \n",
      "EVAL: [0/49] Elapsed 0m 1s (remain 1m 26s) Loss: 1.3218(1.3218) \n",
      "EVAL: [20/49] Elapsed 0m 33s (remain 0m 44s) Loss: 1.4908(1.3632) \n",
      "EVAL: [40/49] Elapsed 1m 1s (remain 0m 12s) Loss: 1.5398(1.3500) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 1.2665  avg_val_loss: 1.3447  time: 2176s\n",
      "Epoch 2 - Score: 0.4988  Scores: [0.549354422326719, 0.4993602044724246, 0.43134832916584803, 0.5163565056438392, 0.5012771412886455, 0.49485847520875453]\n",
      "Epoch 2 - Save Best Score: 0.4988 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [48/49] Elapsed 1m 11s (remain 0m 0s) Loss: 1.1188(1.3447) \n",
      "Epoch: [3][0/880] Elapsed 0m 2s (remain 36m 56s) Loss: 1.1081(1.1081) Grad: 539608.1875  LR: 0.00000999  \n",
      "Epoch: [3][20/880] Elapsed 0m 46s (remain 31m 25s) Loss: 1.1517(1.1097) Grad: 476653.0938  LR: 0.00000981  \n",
      "Epoch: [3][40/880] Elapsed 1m 41s (remain 34m 35s) Loss: 1.3932(1.1660) Grad: 507740.9375  LR: 0.00000963  \n",
      "Epoch: [3][60/880] Elapsed 2m 37s (remain 35m 16s) Loss: 0.9797(1.1511) Grad: 399716.9062  LR: 0.00000946  \n",
      "Epoch: [3][80/880] Elapsed 3m 27s (remain 34m 10s) Loss: 1.0121(1.1450) Grad: 438963.9688  LR: 0.00000928  \n",
      "Epoch: [3][100/880] Elapsed 4m 17s (remain 33m 3s) Loss: 1.3456(1.1292) Grad: 824260.0000  LR: 0.00000910  \n",
      "Epoch: [3][120/880] Elapsed 5m 2s (remain 31m 38s) Loss: 1.0826(1.1289) Grad: 378132.1562  LR: 0.00000892  \n",
      "Epoch: [3][140/880] Elapsed 5m 55s (remain 31m 1s) Loss: 1.1698(1.1263) Grad: 437824.5000  LR: 0.00000874  \n",
      "Epoch: [3][160/880] Elapsed 6m 44s (remain 30m 7s) Loss: 1.0264(1.1271) Grad: 420179.9062  LR: 0.00000857  \n",
      "Epoch: [3][180/880] Elapsed 7m 40s (remain 29m 36s) Loss: 1.2309(1.1328) Grad: nan  LR: 0.00000839  \n",
      "Epoch: [3][200/880] Elapsed 8m 33s (remain 28m 56s) Loss: 0.9796(1.1279) Grad: 198947.0312  LR: 0.00000822  \n",
      "Epoch: [3][220/880] Elapsed 9m 16s (remain 27m 39s) Loss: 1.0887(1.1271) Grad: 174713.3281  LR: 0.00000804  \n",
      "Epoch: [3][240/880] Elapsed 10m 1s (remain 26m 34s) Loss: 1.1609(1.1256) Grad: 296059.7500  LR: 0.00000787  \n",
      "Epoch: [3][260/880] Elapsed 10m 47s (remain 25m 35s) Loss: 0.9569(1.1228) Grad: 246206.3750  LR: 0.00000769  \n",
      "Epoch: [3][280/880] Elapsed 11m 36s (remain 24m 44s) Loss: 1.1858(1.1207) Grad: 197592.3750  LR: 0.00000752  \n",
      "Epoch: [3][300/880] Elapsed 12m 24s (remain 23m 52s) Loss: 1.1316(1.1258) Grad: 266875.7812  LR: 0.00000735  \n",
      "Epoch: [3][320/880] Elapsed 13m 11s (remain 22m 57s) Loss: 0.9772(1.1272) Grad: 228624.5312  LR: 0.00000717  \n",
      "Epoch: [3][340/880] Elapsed 13m 57s (remain 22m 3s) Loss: 1.3124(1.1280) Grad: 324150.9375  LR: 0.00000700  \n",
      "Epoch: [3][360/880] Elapsed 14m 49s (remain 21m 18s) Loss: 1.0136(1.1280) Grad: 289991.2188  LR: 0.00000683  \n",
      "Epoch: [3][380/880] Elapsed 15m 38s (remain 20m 29s) Loss: 1.0888(1.1271) Grad: 276631.6875  LR: 0.00000666  \n",
      "Epoch: [3][400/880] Elapsed 16m 31s (remain 19m 44s) Loss: 0.9777(1.1257) Grad: 191942.3125  LR: 0.00000650  \n",
      "Epoch: [3][420/880] Elapsed 17m 24s (remain 18m 58s) Loss: 1.1474(1.1253) Grad: 194433.4219  LR: 0.00000633  \n",
      "Epoch: [3][440/880] Elapsed 18m 8s (remain 18m 3s) Loss: 1.0301(1.1237) Grad: 196378.4062  LR: 0.00000616  \n",
      "Epoch: [3][460/880] Elapsed 18m 53s (remain 17m 10s) Loss: 1.0943(1.1241) Grad: 208010.5781  LR: 0.00000600  \n",
      "Epoch: [3][480/880] Elapsed 19m 47s (remain 16m 24s) Loss: 1.0743(1.1225) Grad: 259841.6875  LR: 0.00000584  \n",
      "Epoch: [3][500/880] Elapsed 20m 35s (remain 15m 34s) Loss: 0.9489(1.1193) Grad: 202124.3438  LR: 0.00000568  \n",
      "Epoch: [3][520/880] Elapsed 21m 23s (remain 14m 44s) Loss: 1.2233(1.1180) Grad: 259452.6875  LR: 0.00000552  \n",
      "Epoch: [3][540/880] Elapsed 22m 9s (remain 13m 53s) Loss: 1.1928(1.1170) Grad: 241614.8125  LR: 0.00000536  \n",
      "Epoch: [3][560/880] Elapsed 23m 1s (remain 13m 5s) Loss: 1.1991(1.1185) Grad: 292006.5312  LR: 0.00000520  \n",
      "Epoch: [3][580/880] Elapsed 23m 45s (remain 12m 13s) Loss: 1.2012(1.1190) Grad: 233561.1406  LR: 0.00000504  \n",
      "Epoch: [3][600/880] Elapsed 24m 33s (remain 11m 24s) Loss: 1.1866(1.1202) Grad: 250274.2031  LR: 0.00000489  \n",
      "Epoch: [3][620/880] Elapsed 25m 18s (remain 10m 33s) Loss: 1.2049(1.1187) Grad: 226396.2188  LR: 0.00000474  \n",
      "Epoch: [3][640/880] Elapsed 26m 4s (remain 9m 43s) Loss: 1.2785(1.1163) Grad: 305795.7812  LR: 0.00000459  \n",
      "Epoch: [3][660/880] Elapsed 26m 47s (remain 8m 52s) Loss: 1.0883(1.1156) Grad: 265723.4062  LR: 0.00000444  \n",
      "Epoch: [3][680/880] Elapsed 27m 38s (remain 8m 4s) Loss: 1.0354(1.1153) Grad: 198352.2031  LR: 0.00000429  \n",
      "Epoch: [3][700/880] Elapsed 28m 28s (remain 7m 16s) Loss: 0.9746(1.1144) Grad: 266324.6875  LR: 0.00000414  \n",
      "Epoch: [3][720/880] Elapsed 29m 16s (remain 6m 27s) Loss: 1.0136(1.1135) Grad: 190641.2656  LR: 0.00000400  \n",
      "Epoch: [3][740/880] Elapsed 30m 4s (remain 5m 38s) Loss: 1.0302(1.1116) Grad: 223753.9062  LR: 0.00000386  \n",
      "Epoch: [3][760/880] Elapsed 30m 47s (remain 4m 48s) Loss: 1.0602(1.1105) Grad: 175390.1562  LR: 0.00000372  \n",
      "Epoch: [3][780/880] Elapsed 31m 34s (remain 4m 0s) Loss: 0.9810(1.1105) Grad: 201014.4844  LR: 0.00000358  \n",
      "Epoch: [3][800/880] Elapsed 32m 16s (remain 3m 10s) Loss: 1.1685(1.1097) Grad: 266124.0625  LR: 0.00000344  \n",
      "Epoch: [3][820/880] Elapsed 33m 0s (remain 2m 22s) Loss: 0.9797(1.1083) Grad: 258915.4375  LR: 0.00000331  \n",
      "Epoch: [3][840/880] Elapsed 33m 44s (remain 1m 33s) Loss: 1.0957(1.1084) Grad: 236852.3125  LR: 0.00000318  \n",
      "Epoch: [3][860/880] Elapsed 34m 35s (remain 0m 45s) Loss: 1.0775(1.1084) Grad: 341437.0625  LR: 0.00000305  \n",
      "Epoch: [3][879/880] Elapsed 35m 21s (remain 0m 0s) Loss: 0.9509(1.1083) Grad: 170404.7188  LR: 0.00000293  \n",
      "EVAL: [0/49] Elapsed 0m 1s (remain 1m 26s) Loss: 1.2808(1.2808) \n",
      "EVAL: [20/49] Elapsed 0m 33s (remain 0m 44s) Loss: 1.5322(1.3764) \n",
      "EVAL: [40/49] Elapsed 1m 1s (remain 0m 12s) Loss: 1.3865(1.3458) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 1.1083  avg_val_loss: 1.3379  time: 2194s\n",
      "Epoch 3 - Score: 0.4815  Scores: [0.5025510369674241, 0.4662524041201569, 0.4372373160976031, 0.500638977896506, 0.5, 0.48242779033760647]\n",
      "Epoch 3 - Save Best Score: 0.4815 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [48/49] Elapsed 1m 11s (remain 0m 0s) Loss: 1.1520(1.3379) \n",
      "Epoch: [4][0/880] Elapsed 0m 2s (remain 32m 30s) Loss: 0.9455(0.9455) Grad: 331953.8438  LR: 0.00000292  \n",
      "Epoch: [4][20/880] Elapsed 0m 50s (remain 34m 23s) Loss: 0.8126(1.0548) Grad: 363798.5000  LR: 0.00000280  \n",
      "Epoch: [4][40/880] Elapsed 1m 42s (remain 34m 52s) Loss: 1.0223(1.0377) Grad: 342713.5312  LR: 0.00000267  \n",
      "Epoch: [4][60/880] Elapsed 2m 31s (remain 33m 52s) Loss: 0.9925(1.0353) Grad: 445444.9375  LR: 0.00000255  \n",
      "Epoch: [4][80/880] Elapsed 3m 17s (remain 32m 32s) Loss: 1.0964(1.0368) Grad: 508424.6250  LR: 0.00000244  \n",
      "Epoch: [4][100/880] Elapsed 4m 12s (remain 32m 24s) Loss: 0.9932(1.0328) Grad: 477732.3750  LR: 0.00000232  \n",
      "Epoch: [4][120/880] Elapsed 4m 55s (remain 30m 54s) Loss: 0.9807(1.0306) Grad: 389626.9688  LR: 0.00000221  \n",
      "Epoch: [4][140/880] Elapsed 5m 47s (remain 30m 20s) Loss: 1.1666(1.0288) Grad: 521687.5625  LR: 0.00000210  \n",
      "Epoch: [4][160/880] Elapsed 6m 32s (remain 29m 11s) Loss: 0.9559(1.0287) Grad: 382556.7188  LR: 0.00000199  \n",
      "Epoch: [4][180/880] Elapsed 7m 21s (remain 28m 25s) Loss: 1.0492(1.0328) Grad: 487634.0938  LR: 0.00000188  \n",
      "Epoch: [4][200/880] Elapsed 8m 13s (remain 27m 48s) Loss: 1.1192(1.0309) Grad: 853450.3750  LR: 0.00000178  \n",
      "Epoch: [4][220/880] Elapsed 9m 3s (remain 26m 59s) Loss: 0.9344(1.0281) Grad: 357956.4062  LR: 0.00000168  \n",
      "Epoch: [4][240/880] Elapsed 9m 48s (remain 26m 0s) Loss: 0.8656(1.0258) Grad: 196554.5938  LR: 0.00000158  \n",
      "Epoch: [4][260/880] Elapsed 10m 35s (remain 25m 7s) Loss: 0.9454(1.0268) Grad: 176092.0469  LR: 0.00000149  \n",
      "Epoch: [4][280/880] Elapsed 11m 26s (remain 24m 24s) Loss: 0.7673(1.0229) Grad: 224848.6406  LR: 0.00000140  \n",
      "Epoch: [4][300/880] Elapsed 12m 18s (remain 23m 40s) Loss: 1.0776(1.0248) Grad: 281008.4688  LR: 0.00000131  \n",
      "Epoch: [4][320/880] Elapsed 13m 9s (remain 22m 55s) Loss: 1.0505(1.0260) Grad: 236556.6250  LR: 0.00000122  \n",
      "Epoch: [4][340/880] Elapsed 13m 50s (remain 21m 52s) Loss: 0.9613(1.0229) Grad: 214388.1406  LR: 0.00000113  \n",
      "Epoch: [4][360/880] Elapsed 14m 37s (remain 21m 1s) Loss: 0.8517(1.0229) Grad: 186323.2656  LR: 0.00000105  \n",
      "Epoch: [4][380/880] Elapsed 15m 23s (remain 20m 9s) Loss: 0.9660(1.0218) Grad: 194076.2656  LR: 0.00000098  \n",
      "Epoch: [4][400/880] Elapsed 16m 7s (remain 19m 16s) Loss: 1.0447(1.0227) Grad: 192988.4844  LR: 0.00000090  \n",
      "Epoch: [4][420/880] Elapsed 16m 55s (remain 18m 27s) Loss: 1.0960(1.0218) Grad: 274216.0312  LR: 0.00000083  \n",
      "Epoch: [4][440/880] Elapsed 17m 37s (remain 17m 32s) Loss: 0.9954(1.0212) Grad: 189691.2656  LR: 0.00000076  \n",
      "Epoch: [4][460/880] Elapsed 18m 19s (remain 16m 39s) Loss: 0.7664(1.0199) Grad: 187593.5781  LR: 0.00000069  \n",
      "Epoch: [4][480/880] Elapsed 19m 7s (remain 15m 51s) Loss: 1.1087(1.0183) Grad: 244606.1094  LR: 0.00000063  \n",
      "Epoch: [4][500/880] Elapsed 19m 58s (remain 15m 6s) Loss: 0.7887(1.0182) Grad: 174065.9688  LR: 0.00000057  \n",
      "Epoch: [4][520/880] Elapsed 20m 42s (remain 14m 16s) Loss: 1.0301(1.0180) Grad: 197998.3750  LR: 0.00000051  \n",
      "Epoch: [4][540/880] Elapsed 21m 28s (remain 13m 27s) Loss: 1.0867(1.0175) Grad: 284984.0000  LR: 0.00000045  \n",
      "Epoch: [4][560/880] Elapsed 22m 9s (remain 12m 35s) Loss: 0.9705(1.0162) Grad: 223561.8438  LR: 0.00000040  \n",
      "Epoch: [4][580/880] Elapsed 22m 57s (remain 11m 49s) Loss: 1.0927(1.0172) Grad: 208745.8125  LR: 0.00000035  \n",
      "Epoch: [4][600/880] Elapsed 23m 56s (remain 11m 6s) Loss: 0.8167(1.0165) Grad: 174459.0938  LR: 0.00000031  \n",
      "Epoch: [4][620/880] Elapsed 24m 48s (remain 10m 20s) Loss: 1.0175(1.0162) Grad: 197565.8594  LR: 0.00000027  \n",
      "Epoch: [4][640/880] Elapsed 25m 31s (remain 9m 31s) Loss: 1.1244(1.0157) Grad: 241077.1562  LR: 0.00000023  \n",
      "Epoch: [4][660/880] Elapsed 26m 21s (remain 8m 43s) Loss: 1.0203(1.0149) Grad: 310893.3438  LR: 0.00000019  \n",
      "Epoch: [4][680/880] Elapsed 27m 4s (remain 7m 54s) Loss: 0.9464(1.0139) Grad: 180883.8906  LR: 0.00000016  \n",
      "Epoch: [4][700/880] Elapsed 27m 48s (remain 7m 6s) Loss: 1.0484(1.0150) Grad: 286150.3750  LR: 0.00000013  \n",
      "Epoch: [4][720/880] Elapsed 28m 37s (remain 6m 18s) Loss: 1.0966(1.0145) Grad: 198651.9531  LR: 0.00000010  \n",
      "Epoch: [4][740/880] Elapsed 29m 27s (remain 5m 31s) Loss: 0.8820(1.0135) Grad: 195799.1875  LR: 0.00000008  \n",
      "Epoch: [4][760/880] Elapsed 30m 15s (remain 4m 43s) Loss: 0.9448(1.0137) Grad: 222083.1094  LR: 0.00000006  \n",
      "Epoch: [4][780/880] Elapsed 31m 4s (remain 3m 56s) Loss: 1.3514(1.0142) Grad: 203415.3438  LR: 0.00000004  \n",
      "Epoch: [4][800/880] Elapsed 32m 2s (remain 3m 9s) Loss: 1.2542(1.0138) Grad: 467444.7188  LR: 0.00000002  \n",
      "Epoch: [4][820/880] Elapsed 32m 44s (remain 2m 21s) Loss: 0.8766(1.0135) Grad: 291817.0625  LR: 0.00000001  \n",
      "Epoch: [4][840/880] Elapsed 33m 32s (remain 1m 33s) Loss: 1.2150(1.0137) Grad: 332411.5938  LR: 0.00000001  \n",
      "Epoch: [4][860/880] Elapsed 34m 25s (remain 0m 45s) Loss: 1.0411(1.0140) Grad: 381245.8438  LR: 0.00000000  \n",
      "Epoch: [4][879/880] Elapsed 35m 10s (remain 0m 0s) Loss: 1.0133(1.0132) Grad: 206376.0312  LR: 0.00000000  \n",
      "EVAL: [0/49] Elapsed 0m 1s (remain 1m 25s) Loss: 1.3464(1.3464) \n",
      "EVAL: [20/49] Elapsed 0m 33s (remain 0m 44s) Loss: 1.5442(1.3895) \n",
      "EVAL: [40/49] Elapsed 1m 1s (remain 0m 12s) Loss: 1.3562(1.3569) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 1.0132  avg_val_loss: 1.3496  time: 2183s\n",
      "Epoch 4 - Score: 0.4802  Scores: [0.49161771686915634, 0.46004002939364014, 0.43942533968355785, 0.5019144932832433, 0.496792782811615, 0.49161771686915634]\n",
      "Epoch 4 - Save Best Score: 0.4802 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [48/49] Elapsed 1m 11s (remain 0m 0s) Loss: 1.1420(1.3496) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 7 result ==========\n",
      "Score: 0.4802  Scores: [0.49161771686915634, 0.46004002939364014, 0.43942533968355785, 0.5019144932832433, 0.496792782811615, 0.49161771686915634]\n",
      "========== fold: 8 training ==========\n",
      "DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-large\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 1024,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/880] Elapsed 0m 3s (remain 48m 21s) Loss: 2.2211(2.2211) Grad: inf  LR: 0.00002000  \n",
      "Epoch: [1][20/880] Elapsed 0m 52s (remain 35m 35s) Loss: 1.5274(1.9497) Grad: 110629.3281  LR: 0.00002000  \n",
      "Epoch: [1][40/880] Elapsed 1m 38s (remain 33m 35s) Loss: 1.8122(1.8650) Grad: 125074.3047  LR: 0.00001999  \n",
      "Epoch: [1][60/880] Elapsed 2m 34s (remain 34m 28s) Loss: 1.9077(1.7935) Grad: 122157.9219  LR: 0.00001999  \n",
      "Epoch: [1][80/880] Elapsed 3m 23s (remain 33m 31s) Loss: 1.3543(1.7411) Grad: 153706.3594  LR: 0.00001997  \n",
      "Epoch: [1][100/880] Elapsed 4m 6s (remain 31m 37s) Loss: 1.2976(1.6867) Grad: 101130.1797  LR: 0.00001996  \n",
      "Epoch: [1][120/880] Elapsed 4m 59s (remain 31m 17s) Loss: 1.2916(1.6655) Grad: 186630.5469  LR: 0.00001994  \n",
      "Epoch: [1][140/880] Elapsed 5m 42s (remain 29m 53s) Loss: 1.4255(1.6386) Grad: 142278.6562  LR: 0.00001992  \n",
      "Epoch: [1][160/880] Elapsed 6m 32s (remain 29m 14s) Loss: 1.6381(1.6180) Grad: 110599.7500  LR: 0.00001990  \n",
      "Epoch: [1][180/880] Elapsed 7m 10s (remain 27m 41s) Loss: 1.5706(1.6005) Grad: 126475.3516  LR: 0.00001987  \n",
      "Epoch: [1][200/880] Elapsed 8m 1s (remain 27m 7s) Loss: 1.7851(1.5870) Grad: 139975.2656  LR: 0.00001984  \n",
      "Epoch: [1][220/880] Elapsed 8m 47s (remain 26m 13s) Loss: 1.3989(1.5816) Grad: 114794.4375  LR: 0.00001981  \n",
      "Epoch: [1][240/880] Elapsed 9m 28s (remain 25m 8s) Loss: 1.3411(1.5631) Grad: 110103.0781  LR: 0.00001977  \n",
      "Epoch: [1][260/880] Elapsed 10m 24s (remain 24m 40s) Loss: 1.0709(1.5555) Grad: 118353.6562  LR: 0.00001973  \n",
      "Epoch: [1][280/880] Elapsed 11m 6s (remain 23m 40s) Loss: 1.8668(1.5499) Grad: 114175.6328  LR: 0.00001969  \n",
      "Epoch: [1][300/880] Elapsed 11m 48s (remain 22m 42s) Loss: 1.5000(1.5429) Grad: 108850.0469  LR: 0.00001964  \n",
      "Epoch: [1][320/880] Elapsed 12m 33s (remain 21m 52s) Loss: 1.7769(1.5385) Grad: 127263.9922  LR: 0.00001959  \n",
      "Epoch: [1][340/880] Elapsed 13m 19s (remain 21m 3s) Loss: 1.2929(1.5300) Grad: 185416.6562  LR: 0.00001954  \n",
      "Epoch: [1][360/880] Elapsed 14m 7s (remain 20m 18s) Loss: 1.0785(1.5209) Grad: 80592.0469  LR: 0.00001949  \n",
      "Epoch: [1][380/880] Elapsed 14m 51s (remain 19m 27s) Loss: 1.4119(1.5121) Grad: 173797.0625  LR: 0.00001943  \n",
      "Epoch: [1][400/880] Elapsed 15m 38s (remain 18m 41s) Loss: 1.5094(1.5106) Grad: 111300.1562  LR: 0.00001937  \n",
      "Epoch: [1][420/880] Elapsed 16m 25s (remain 17m 54s) Loss: 1.0766(1.5030) Grad: 92803.0000  LR: 0.00001930  \n",
      "Epoch: [1][440/880] Elapsed 17m 11s (remain 17m 6s) Loss: 1.2069(1.4962) Grad: 102428.5234  LR: 0.00001924  \n",
      "Epoch: [1][460/880] Elapsed 17m 54s (remain 16m 16s) Loss: 1.4253(1.4946) Grad: 98109.4844  LR: 0.00001917  \n",
      "Epoch: [1][480/880] Elapsed 18m 45s (remain 15m 33s) Loss: 1.1254(1.4930) Grad: 96083.1562  LR: 0.00001909  \n",
      "Epoch: [1][500/880] Elapsed 19m 34s (remain 14m 48s) Loss: 1.6766(1.4907) Grad: 158012.6562  LR: 0.00001902  \n",
      "Epoch: [1][520/880] Elapsed 20m 25s (remain 14m 4s) Loss: 1.3302(1.4892) Grad: 97624.2109  LR: 0.00001894  \n",
      "Epoch: [1][540/880] Elapsed 21m 21s (remain 13m 22s) Loss: 1.4776(1.4861) Grad: 134743.7812  LR: 0.00001886  \n",
      "Epoch: [1][560/880] Elapsed 22m 9s (remain 12m 36s) Loss: 1.3669(1.4836) Grad: 101006.0000  LR: 0.00001877  \n",
      "Epoch: [1][580/880] Elapsed 22m 52s (remain 11m 46s) Loss: 1.4109(1.4807) Grad: 122296.6094  LR: 0.00001869  \n",
      "Epoch: [1][600/880] Elapsed 23m 41s (remain 10m 59s) Loss: 1.3254(1.4775) Grad: 130971.2344  LR: 0.00001860  \n",
      "Epoch: [1][620/880] Elapsed 24m 28s (remain 10m 12s) Loss: 1.4113(1.4747) Grad: 143960.2500  LR: 0.00001850  \n",
      "Epoch: [1][640/880] Elapsed 25m 19s (remain 9m 26s) Loss: 1.5200(1.4718) Grad: 111379.2891  LR: 0.00001841  \n",
      "Epoch: [1][660/880] Elapsed 26m 6s (remain 8m 39s) Loss: 1.1285(1.4706) Grad: 110480.0703  LR: 0.00001831  \n",
      "Epoch: [1][680/880] Elapsed 26m 56s (remain 7m 52s) Loss: 1.2640(1.4688) Grad: 83820.9844  LR: 0.00001821  \n",
      "Epoch: [1][700/880] Elapsed 27m 44s (remain 7m 5s) Loss: 1.6677(1.4660) Grad: 171404.2344  LR: 0.00001811  \n",
      "Epoch: [1][720/880] Elapsed 28m 40s (remain 6m 19s) Loss: 1.4848(1.4655) Grad: 108637.0000  LR: 0.00001800  \n",
      "Epoch: [1][740/880] Elapsed 29m 30s (remain 5m 32s) Loss: 1.3454(1.4635) Grad: 91851.3438  LR: 0.00001789  \n",
      "Epoch: [1][760/880] Elapsed 30m 16s (remain 4m 44s) Loss: 1.9598(1.4616) Grad: 146525.1875  LR: 0.00001778  \n",
      "Epoch: [1][780/880] Elapsed 31m 3s (remain 3m 56s) Loss: 1.1640(1.4589) Grad: 85342.8672  LR: 0.00001767  \n",
      "Epoch: [1][800/880] Elapsed 31m 52s (remain 3m 8s) Loss: 1.3851(1.4582) Grad: 118229.5625  LR: 0.00001755  \n",
      "Epoch: [1][820/880] Elapsed 32m 35s (remain 2m 20s) Loss: 2.6856(1.4576) Grad: 141995.2188  LR: 0.00001743  \n",
      "Epoch: [1][840/880] Elapsed 33m 19s (remain 1m 32s) Loss: 1.2696(1.4541) Grad: 104519.2969  LR: 0.00001731  \n",
      "Epoch: [1][860/880] Elapsed 34m 4s (remain 0m 45s) Loss: 1.2363(1.4517) Grad: 103237.8984  LR: 0.00001719  \n",
      "Epoch: [1][879/880] Elapsed 34m 47s (remain 0m 0s) Loss: 1.0218(1.4495) Grad: 101773.1484  LR: 0.00001707  \n",
      "EVAL: [0/49] Elapsed 0m 1s (remain 1m 8s) Loss: 1.2740(1.2740) \n",
      "EVAL: [20/49] Elapsed 0m 32s (remain 0m 43s) Loss: 1.7912(1.4737) \n",
      "EVAL: [40/49] Elapsed 1m 5s (remain 0m 12s) Loss: 1.4523(1.4613) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 1.4495  avg_val_loss: 1.4400  time: 2165s\n",
      "Epoch 1 - Score: 0.5294  Scores: [0.5637152969907603, 0.5206723962776636, 0.4791029554738042, 0.5522564732768778, 0.547605809649741, 0.5132515075227978]\n",
      "Epoch 1 - Save Best Score: 0.5294 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [48/49] Elapsed 1m 16s (remain 0m 0s) Loss: 1.2207(1.4400) \n",
      "Epoch: [2][0/880] Elapsed 0m 2s (remain 35m 40s) Loss: 1.2215(1.2215) Grad: 276979.4375  LR: 0.00001706  \n",
      "Epoch: [2][20/880] Elapsed 0m 50s (remain 34m 36s) Loss: 1.4461(1.3371) Grad: 300715.4375  LR: 0.00001694  \n",
      "Epoch: [2][40/880] Elapsed 1m 47s (remain 36m 36s) Loss: 1.3851(1.3255) Grad: 199553.1406  LR: 0.00001681  \n",
      "Epoch: [2][60/880] Elapsed 2m 37s (remain 35m 10s) Loss: 1.1914(1.3064) Grad: 188385.5156  LR: 0.00001668  \n",
      "Epoch: [2][80/880] Elapsed 3m 18s (remain 32m 39s) Loss: 1.4098(1.3125) Grad: 285217.0625  LR: 0.00001654  \n",
      "Epoch: [2][100/880] Elapsed 4m 14s (remain 32m 46s) Loss: 1.4036(1.3050) Grad: 211192.7500  LR: 0.00001641  \n",
      "Epoch: [2][120/880] Elapsed 5m 5s (remain 31m 56s) Loss: 1.2666(1.2918) Grad: 222297.9844  LR: 0.00001627  \n",
      "Epoch: [2][140/880] Elapsed 5m 51s (remain 30m 42s) Loss: 1.1033(1.2902) Grad: 236691.5156  LR: 0.00001613  \n",
      "Epoch: [2][160/880] Elapsed 6m 43s (remain 30m 3s) Loss: 1.0607(1.2807) Grad: 193663.8750  LR: 0.00001599  \n",
      "Epoch: [2][180/880] Elapsed 7m 26s (remain 28m 46s) Loss: 1.2044(1.2752) Grad: 209980.5312  LR: 0.00001584  \n",
      "Epoch: [2][200/880] Elapsed 8m 14s (remain 27m 49s) Loss: 1.0166(1.2694) Grad: 215637.1719  LR: 0.00001570  \n",
      "Epoch: [2][220/880] Elapsed 9m 7s (remain 27m 11s) Loss: 1.2011(1.2689) Grad: 210093.9219  LR: 0.00001555  \n",
      "Epoch: [2][240/880] Elapsed 9m 54s (remain 26m 16s) Loss: 1.1548(1.2641) Grad: 264864.0000  LR: 0.00001540  \n",
      "Epoch: [2][260/880] Elapsed 10m 42s (remain 25m 23s) Loss: 1.3235(1.2620) Grad: 225383.5156  LR: 0.00001525  \n",
      "Epoch: [2][280/880] Elapsed 11m 28s (remain 24m 28s) Loss: 1.2456(1.2638) Grad: 205667.3750  LR: 0.00001510  \n",
      "Epoch: [2][300/880] Elapsed 12m 10s (remain 23m 25s) Loss: 0.9917(1.2637) Grad: 181140.2812  LR: 0.00001494  \n",
      "Epoch: [2][320/880] Elapsed 12m 57s (remain 22m 33s) Loss: 1.2329(1.2636) Grad: 229099.8906  LR: 0.00001478  \n",
      "Epoch: [2][340/880] Elapsed 13m 43s (remain 21m 42s) Loss: 1.3967(1.2610) Grad: 279661.5625  LR: 0.00001463  \n",
      "Epoch: [2][360/880] Elapsed 14m 31s (remain 20m 53s) Loss: 1.1640(1.2597) Grad: 214441.2500  LR: 0.00001447  \n",
      "Epoch: [2][380/880] Elapsed 15m 21s (remain 20m 7s) Loss: 1.5524(1.2571) Grad: 266584.3750  LR: 0.00001431  \n",
      "Epoch: [2][400/880] Elapsed 16m 2s (remain 19m 9s) Loss: 1.4425(1.2543) Grad: 322412.6875  LR: 0.00001415  \n",
      "Epoch: [2][420/880] Elapsed 16m 48s (remain 18m 19s) Loss: 1.2757(1.2563) Grad: 236008.3594  LR: 0.00001398  \n",
      "Epoch: [2][440/880] Elapsed 17m 34s (remain 17m 29s) Loss: 1.4768(1.2596) Grad: 219298.4844  LR: 0.00001382  \n",
      "Epoch: [2][460/880] Elapsed 18m 16s (remain 16m 36s) Loss: 1.0175(1.2615) Grad: 157787.0938  LR: 0.00001365  \n",
      "Epoch: [2][480/880] Elapsed 19m 5s (remain 15m 50s) Loss: 1.1534(1.2591) Grad: 259925.7344  LR: 0.00001349  \n",
      "Epoch: [2][500/880] Elapsed 19m 56s (remain 15m 5s) Loss: 1.5984(1.2586) Grad: 588662.7500  LR: 0.00001332  \n",
      "Epoch: [2][520/880] Elapsed 20m 44s (remain 14m 17s) Loss: 1.1521(1.2584) Grad: 195856.6406  LR: 0.00001315  \n",
      "Epoch: [2][540/880] Elapsed 21m 34s (remain 13m 30s) Loss: 1.3063(1.2579) Grad: 320988.1562  LR: 0.00001298  \n",
      "Epoch: [2][560/880] Elapsed 22m 13s (remain 12m 38s) Loss: 1.2230(1.2553) Grad: 235700.2344  LR: 0.00001281  \n",
      "Epoch: [2][580/880] Elapsed 23m 2s (remain 11m 51s) Loss: 1.2233(1.2548) Grad: 276077.3438  LR: 0.00001264  \n",
      "Epoch: [2][600/880] Elapsed 23m 51s (remain 11m 4s) Loss: 1.1917(1.2536) Grad: 194986.2656  LR: 0.00001246  \n",
      "Epoch: [2][620/880] Elapsed 24m 40s (remain 10m 17s) Loss: 1.4008(1.2535) Grad: 354261.9375  LR: 0.00001229  \n",
      "Epoch: [2][640/880] Elapsed 25m 32s (remain 9m 31s) Loss: 1.1858(1.2517) Grad: 251948.7656  LR: 0.00001212  \n",
      "Epoch: [2][660/880] Elapsed 26m 19s (remain 8m 43s) Loss: 1.3077(1.2505) Grad: 285361.6250  LR: 0.00001194  \n",
      "Epoch: [2][680/880] Elapsed 27m 3s (remain 7m 54s) Loss: 0.9128(1.2488) Grad: 188482.1719  LR: 0.00001177  \n",
      "Epoch: [2][700/880] Elapsed 27m 52s (remain 7m 7s) Loss: 1.1703(1.2474) Grad: 196321.3906  LR: 0.00001159  \n",
      "Epoch: [2][720/880] Elapsed 28m 38s (remain 6m 19s) Loss: 1.4800(1.2474) Grad: 254690.4062  LR: 0.00001141  \n",
      "Epoch: [2][740/880] Elapsed 29m 22s (remain 5m 30s) Loss: 1.4475(1.2472) Grad: 248445.3750  LR: 0.00001124  \n",
      "Epoch: [2][760/880] Elapsed 30m 6s (remain 4m 42s) Loss: 1.2077(1.2460) Grad: 231306.6719  LR: 0.00001106  \n",
      "Epoch: [2][780/880] Elapsed 30m 53s (remain 3m 54s) Loss: 1.1138(1.2458) Grad: 291442.9062  LR: 0.00001088  \n",
      "Epoch: [2][800/880] Elapsed 31m 39s (remain 3m 7s) Loss: 1.1052(1.2457) Grad: 225344.2031  LR: 0.00001070  \n",
      "Epoch: [2][820/880] Elapsed 32m 33s (remain 2m 20s) Loss: 1.0100(1.2449) Grad: 172129.7812  LR: 0.00001053  \n",
      "Epoch: [2][840/880] Elapsed 33m 28s (remain 1m 33s) Loss: 1.5507(1.2468) Grad: 289546.5000  LR: 0.00001035  \n",
      "Epoch: [2][860/880] Elapsed 34m 11s (remain 0m 45s) Loss: 1.1651(1.2470) Grad: 202245.5625  LR: 0.00001017  \n",
      "Epoch: [2][879/880] Elapsed 34m 52s (remain 0m 0s) Loss: 1.1563(1.2461) Grad: 175627.7031  LR: 0.00001000  \n",
      "EVAL: [0/49] Elapsed 0m 1s (remain 1m 9s) Loss: 1.1783(1.1783) \n",
      "EVAL: [20/49] Elapsed 0m 32s (remain 0m 43s) Loss: 1.3908(1.3208) \n",
      "EVAL: [40/49] Elapsed 1m 5s (remain 0m 12s) Loss: 1.2030(1.3179) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 1.2461  avg_val_loss: 1.3139  time: 2169s\n",
      "Epoch 2 - Score: 0.4875  Scores: [0.5144957554275266, 0.49807814791679533, 0.4116276717537983, 0.5101276105330244, 0.5163565056438392, 0.474409041459926]\n",
      "Epoch 2 - Save Best Score: 0.4875 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [48/49] Elapsed 1m 16s (remain 0m 0s) Loss: 1.2015(1.3139) \n",
      "Epoch: [3][0/880] Elapsed 0m 1s (remain 23m 5s) Loss: 0.9038(0.9038) Grad: 375303.3125  LR: 0.00000999  \n",
      "Epoch: [3][20/880] Elapsed 0m 52s (remain 35m 39s) Loss: 1.0933(1.0865) Grad: 350975.6562  LR: 0.00000981  \n",
      "Epoch: [3][40/880] Elapsed 1m 33s (remain 31m 43s) Loss: 1.0238(1.0957) Grad: 652998.3750  LR: 0.00000963  \n",
      "Epoch: [3][60/880] Elapsed 2m 14s (remain 30m 7s) Loss: 1.1743(1.1026) Grad: 420410.5938  LR: 0.00000946  \n",
      "Epoch: [3][80/880] Elapsed 2m 59s (remain 29m 29s) Loss: 1.0218(1.1083) Grad: 382134.8438  LR: 0.00000928  \n",
      "Epoch: [3][100/880] Elapsed 3m 38s (remain 28m 5s) Loss: 1.0810(1.1002) Grad: 358513.2188  LR: 0.00000910  \n",
      "Epoch: [3][120/880] Elapsed 4m 26s (remain 27m 52s) Loss: 0.9489(1.1053) Grad: 423241.2188  LR: 0.00000892  \n",
      "Epoch: [3][140/880] Elapsed 5m 12s (remain 27m 20s) Loss: 1.2834(1.1098) Grad: 739431.9375  LR: 0.00000874  \n",
      "Epoch: [3][160/880] Elapsed 5m 54s (remain 26m 23s) Loss: 1.0287(1.1102) Grad: 451839.5625  LR: 0.00000857  \n",
      "Epoch: [3][180/880] Elapsed 6m 36s (remain 25m 29s) Loss: 1.2267(1.1059) Grad: 659477.4375  LR: 0.00000839  \n",
      "Epoch: [3][200/880] Elapsed 7m 29s (remain 25m 19s) Loss: 0.8330(1.1034) Grad: 348650.4375  LR: 0.00000822  \n",
      "Epoch: [3][220/880] Elapsed 8m 13s (remain 24m 30s) Loss: 1.2408(1.1033) Grad: 454391.8750  LR: 0.00000804  \n",
      "Epoch: [3][240/880] Elapsed 8m 55s (remain 23m 40s) Loss: 1.0054(1.1030) Grad: 450079.3438  LR: 0.00000787  \n",
      "Epoch: [3][260/880] Elapsed 9m 37s (remain 22m 49s) Loss: 1.0605(1.0995) Grad: 410265.0938  LR: 0.00000769  \n",
      "Epoch: [3][280/880] Elapsed 10m 22s (remain 22m 7s) Loss: 1.1073(1.1004) Grad: 550470.7500  LR: 0.00000752  \n",
      "Epoch: [3][300/880] Elapsed 11m 16s (remain 21m 42s) Loss: 0.9883(1.1007) Grad: 359799.9375  LR: 0.00000735  \n",
      "Epoch: [3][320/880] Elapsed 12m 9s (remain 21m 10s) Loss: 0.9079(1.1017) Grad: 423918.0938  LR: 0.00000717  \n",
      "Epoch: [3][340/880] Elapsed 12m 59s (remain 20m 31s) Loss: 0.8615(1.0989) Grad: 374624.8750  LR: 0.00000700  \n",
      "Epoch: [3][360/880] Elapsed 13m 56s (remain 20m 2s) Loss: 1.1130(1.0987) Grad: 831726.0625  LR: 0.00000683  \n",
      "Epoch: [3][380/880] Elapsed 14m 39s (remain 19m 11s) Loss: 1.1997(1.0989) Grad: 638619.0625  LR: 0.00000666  \n",
      "Epoch: [3][400/880] Elapsed 15m 38s (remain 18m 40s) Loss: 1.1339(1.0979) Grad: 685406.5000  LR: 0.00000650  \n",
      "Epoch: [3][420/880] Elapsed 16m 26s (remain 17m 55s) Loss: 1.1881(1.0985) Grad: 465492.6250  LR: 0.00000633  \n",
      "Epoch: [3][440/880] Elapsed 17m 12s (remain 17m 7s) Loss: 1.0569(1.0984) Grad: 450381.9375  LR: 0.00000616  \n",
      "Epoch: [3][460/880] Elapsed 18m 5s (remain 16m 26s) Loss: 1.1584(1.0976) Grad: 449045.2812  LR: 0.00000600  \n",
      "Epoch: [3][480/880] Elapsed 18m 57s (remain 15m 43s) Loss: 1.0591(1.0970) Grad: 450738.9375  LR: 0.00000584  \n",
      "Epoch: [3][500/880] Elapsed 19m 48s (remain 14m 59s) Loss: 1.0103(1.0984) Grad: 364060.1250  LR: 0.00000568  \n",
      "Epoch: [3][520/880] Elapsed 20m 44s (remain 14m 17s) Loss: 1.6341(1.0999) Grad: 547052.1250  LR: 0.00000552  \n",
      "Epoch: [3][540/880] Elapsed 21m 27s (remain 13m 26s) Loss: 1.0287(1.0991) Grad: 441943.4062  LR: 0.00000536  \n",
      "Epoch: [3][560/880] Elapsed 22m 13s (remain 12m 38s) Loss: 1.1012(1.0981) Grad: 466324.9062  LR: 0.00000520  \n",
      "Epoch: [3][580/880] Elapsed 23m 7s (remain 11m 54s) Loss: 1.1190(1.0971) Grad: 447018.7188  LR: 0.00000504  \n",
      "Epoch: [3][600/880] Elapsed 24m 2s (remain 11m 9s) Loss: 1.3661(1.0963) Grad: 644694.6875  LR: 0.00000489  \n",
      "Epoch: [3][620/880] Elapsed 24m 54s (remain 10m 23s) Loss: 1.3101(1.0952) Grad: 839892.2500  LR: 0.00000474  \n",
      "Epoch: [3][640/880] Elapsed 25m 42s (remain 9m 35s) Loss: 1.3952(1.0958) Grad: 775980.4375  LR: 0.00000459  \n",
      "Epoch: [3][660/880] Elapsed 26m 27s (remain 8m 45s) Loss: 1.3128(1.0945) Grad: 580764.7500  LR: 0.00000444  \n",
      "Epoch: [3][680/880] Elapsed 27m 10s (remain 7m 56s) Loss: 1.0410(1.0947) Grad: 560298.1875  LR: 0.00000429  \n",
      "Epoch: [3][700/880] Elapsed 27m 52s (remain 7m 7s) Loss: 0.8252(1.0945) Grad: 373940.5938  LR: 0.00000414  \n",
      "Epoch: [3][720/880] Elapsed 28m 33s (remain 6m 17s) Loss: 1.3146(1.0954) Grad: 531031.8750  LR: 0.00000400  \n",
      "Epoch: [3][740/880] Elapsed 29m 13s (remain 5m 28s) Loss: 0.9394(1.0960) Grad: 313065.4375  LR: 0.00000386  \n",
      "Epoch: [3][760/880] Elapsed 30m 0s (remain 4m 41s) Loss: 1.0129(1.0955) Grad: 452742.8125  LR: 0.00000372  \n",
      "Epoch: [3][780/880] Elapsed 30m 46s (remain 3m 54s) Loss: 1.2142(1.0958) Grad: 632666.2500  LR: 0.00000358  \n",
      "Epoch: [3][800/880] Elapsed 31m 29s (remain 3m 6s) Loss: 0.8277(1.0947) Grad: 308711.5312  LR: 0.00000344  \n",
      "Epoch: [3][820/880] Elapsed 32m 17s (remain 2m 19s) Loss: 1.2786(1.0944) Grad: 713498.6250  LR: 0.00000331  \n",
      "Epoch: [3][840/880] Elapsed 33m 14s (remain 1m 32s) Loss: 1.3964(1.0944) Grad: 1099677.6250  LR: 0.00000318  \n",
      "Epoch: [3][860/880] Elapsed 34m 5s (remain 0m 45s) Loss: 1.1335(1.0937) Grad: 538757.3750  LR: 0.00000305  \n",
      "Epoch: [3][879/880] Elapsed 34m 43s (remain 0m 0s) Loss: 1.3667(1.0926) Grad: 523293.1562  LR: 0.00000293  \n",
      "EVAL: [0/49] Elapsed 0m 1s (remain 1m 9s) Loss: 1.1486(1.1486) \n",
      "EVAL: [20/49] Elapsed 0m 32s (remain 0m 43s) Loss: 1.3575(1.3229) \n",
      "EVAL: [40/49] Elapsed 1m 5s (remain 0m 12s) Loss: 1.2134(1.3183) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 1.0926  avg_val_loss: 1.3151  time: 2160s\n",
      "Epoch 3 - Score: 0.4732  Scores: [0.4942120236261712, 0.4586480745255393, 0.42687827766076236, 0.4877003817961311, 0.5120042359159814, 0.46004002939364014]\n",
      "Epoch 3 - Save Best Score: 0.4732 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [48/49] Elapsed 1m 16s (remain 0m 0s) Loss: 1.2563(1.3151) \n",
      "Epoch: [4][0/880] Elapsed 0m 2s (remain 30m 32s) Loss: 0.8785(0.8785) Grad: 500399.5938  LR: 0.00000292  \n",
      "Epoch: [4][20/880] Elapsed 0m 44s (remain 30m 36s) Loss: 0.9833(0.9753) Grad: 483158.0312  LR: 0.00000280  \n",
      "Epoch: [4][40/880] Elapsed 1m 28s (remain 30m 20s) Loss: 0.9999(1.0017) Grad: 467141.6875  LR: 0.00000267  \n",
      "Epoch: [4][60/880] Elapsed 2m 17s (remain 30m 52s) Loss: 0.9756(1.0114) Grad: 379894.2812  LR: 0.00000255  \n",
      "Epoch: [4][80/880] Elapsed 3m 10s (remain 31m 17s) Loss: 0.8506(1.0145) Grad: 359246.5312  LR: 0.00000244  \n",
      "Epoch: [4][100/880] Elapsed 4m 3s (remain 31m 15s) Loss: 0.8331(1.0236) Grad: 441365.4688  LR: 0.00000232  \n",
      "Epoch: [4][120/880] Elapsed 4m 53s (remain 30m 42s) Loss: 1.0909(1.0301) Grad: 546352.5625  LR: 0.00000221  \n",
      "Epoch: [4][140/880] Elapsed 5m 38s (remain 29m 32s) Loss: 1.2633(1.0352) Grad: 677458.2500  LR: 0.00000210  \n",
      "Epoch: [4][160/880] Elapsed 6m 29s (remain 29m 0s) Loss: 0.9326(1.0303) Grad: 355188.8750  LR: 0.00000199  \n",
      "Epoch: [4][180/880] Elapsed 7m 12s (remain 27m 50s) Loss: 1.0080(1.0317) Grad: 413746.6562  LR: 0.00000188  \n",
      "Epoch: [4][200/880] Elapsed 8m 2s (remain 27m 10s) Loss: 0.9285(1.0276) Grad: 331704.1875  LR: 0.00000178  \n",
      "Epoch: [4][220/880] Elapsed 8m 50s (remain 26m 20s) Loss: 1.1508(1.0300) Grad: 524605.3750  LR: 0.00000168  \n",
      "Epoch: [4][240/880] Elapsed 9m 42s (remain 25m 43s) Loss: 1.1382(1.0307) Grad: 514755.6875  LR: 0.00000158  \n",
      "Epoch: [4][260/880] Elapsed 10m 27s (remain 24m 47s) Loss: 0.9481(1.0295) Grad: 446644.0000  LR: 0.00000149  \n",
      "Epoch: [4][280/880] Elapsed 11m 10s (remain 23m 50s) Loss: 1.1593(1.0268) Grad: 556957.2500  LR: 0.00000140  \n",
      "Epoch: [4][300/880] Elapsed 12m 7s (remain 23m 20s) Loss: 0.8478(1.0280) Grad: 437277.4375  LR: 0.00000131  \n",
      "Epoch: [4][320/880] Elapsed 13m 5s (remain 22m 48s) Loss: 0.9296(1.0299) Grad: 378778.6250  LR: 0.00000122  \n",
      "Epoch: [4][340/880] Elapsed 13m 57s (remain 22m 4s) Loss: 1.1611(1.0335) Grad: 820994.5625  LR: 0.00000113  \n",
      "Epoch: [4][360/880] Elapsed 14m 40s (remain 21m 5s) Loss: 0.9912(1.0356) Grad: 542751.8125  LR: 0.00000105  \n",
      "Epoch: [4][380/880] Elapsed 15m 22s (remain 20m 7s) Loss: 1.2507(1.0331) Grad: 420715.0000  LR: 0.00000098  \n",
      "Epoch: [4][400/880] Elapsed 16m 13s (remain 19m 22s) Loss: 1.1656(1.0343) Grad: 445075.6875  LR: 0.00000090  \n",
      "Epoch: [4][420/880] Elapsed 17m 2s (remain 18m 34s) Loss: 0.8741(1.0319) Grad: 367411.1875  LR: 0.00000083  \n",
      "Epoch: [4][440/880] Elapsed 18m 1s (remain 17m 56s) Loss: 0.9693(1.0322) Grad: 484328.0938  LR: 0.00000076  \n",
      "Epoch: [4][460/880] Elapsed 18m 45s (remain 17m 2s) Loss: 0.8580(1.0317) Grad: 612843.6875  LR: 0.00000069  \n",
      "Epoch: [4][480/880] Elapsed 19m 28s (remain 16m 9s) Loss: 0.9348(1.0311) Grad: 415728.3438  LR: 0.00000063  \n",
      "Epoch: [4][500/880] Elapsed 20m 8s (remain 15m 14s) Loss: 1.1480(1.0293) Grad: 452401.4688  LR: 0.00000057  \n",
      "Epoch: [4][520/880] Elapsed 20m 51s (remain 14m 22s) Loss: 1.0484(1.0290) Grad: 330924.9688  LR: 0.00000051  \n",
      "Epoch: [4][540/880] Elapsed 21m 48s (remain 13m 39s) Loss: 1.0876(1.0282) Grad: 562357.3750  LR: 0.00000045  \n",
      "Epoch: [4][560/880] Elapsed 22m 40s (remain 12m 53s) Loss: 1.0998(1.0271) Grad: 727577.8750  LR: 0.00000040  \n",
      "Epoch: [4][580/880] Elapsed 23m 19s (remain 11m 59s) Loss: 1.0347(1.0265) Grad: 543163.8750  LR: 0.00000035  \n",
      "Epoch: [4][600/880] Elapsed 24m 7s (remain 11m 12s) Loss: 1.0614(1.0249) Grad: 387702.5938  LR: 0.00000031  \n",
      "Epoch: [4][620/880] Elapsed 25m 3s (remain 10m 26s) Loss: 1.0110(1.0239) Grad: 618952.4375  LR: 0.00000027  \n",
      "Epoch: [4][640/880] Elapsed 25m 49s (remain 9m 37s) Loss: 1.1398(1.0236) Grad: 534057.3125  LR: 0.00000023  \n",
      "Epoch: [4][660/880] Elapsed 26m 33s (remain 8m 47s) Loss: 0.8612(1.0219) Grad: 402045.6875  LR: 0.00000019  \n",
      "Epoch: [4][680/880] Elapsed 27m 15s (remain 7m 58s) Loss: 1.0272(1.0214) Grad: 417694.5938  LR: 0.00000016  \n",
      "Epoch: [4][700/880] Elapsed 27m 58s (remain 7m 8s) Loss: 0.9222(1.0210) Grad: 347075.1562  LR: 0.00000013  \n",
      "Epoch: [4][720/880] Elapsed 28m 52s (remain 6m 22s) Loss: 0.8481(1.0192) Grad: 414005.9375  LR: 0.00000010  \n",
      "Epoch: [4][740/880] Elapsed 29m 48s (remain 5m 35s) Loss: 0.9348(1.0194) Grad: 376422.4375  LR: 0.00000008  \n",
      "Epoch: [4][760/880] Elapsed 30m 33s (remain 4m 46s) Loss: 0.8957(1.0185) Grad: 331398.9062  LR: 0.00000006  \n",
      "Epoch: [4][780/880] Elapsed 31m 21s (remain 3m 58s) Loss: 1.0155(1.0181) Grad: 418172.6250  LR: 0.00000004  \n",
      "Epoch: [4][800/880] Elapsed 32m 8s (remain 3m 10s) Loss: 0.8993(1.0167) Grad: 427640.6562  LR: 0.00000002  \n",
      "Epoch: [4][820/880] Elapsed 32m 57s (remain 2m 22s) Loss: 0.8660(1.0163) Grad: 412398.3750  LR: 0.00000001  \n",
      "Epoch: [4][840/880] Elapsed 33m 39s (remain 1m 33s) Loss: 1.0122(1.0159) Grad: 535941.3750  LR: 0.00000001  \n",
      "Epoch: [4][860/880] Elapsed 34m 21s (remain 0m 45s) Loss: 1.0032(1.0162) Grad: 482898.2188  LR: 0.00000000  \n",
      "Epoch: [4][879/880] Elapsed 35m 15s (remain 0m 0s) Loss: 0.8783(1.0169) Grad: 499478.6875  LR: 0.00000000  \n",
      "EVAL: [0/49] Elapsed 0m 1s (remain 1m 9s) Loss: 1.1856(1.1856) \n",
      "EVAL: [20/49] Elapsed 0m 32s (remain 0m 43s) Loss: 1.3294(1.3290) \n",
      "EVAL: [40/49] Elapsed 1m 5s (remain 0m 12s) Loss: 1.1946(1.3278) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 1.0169  avg_val_loss: 1.3270  time: 2192s\n",
      "Epoch 4 - Score: 0.4760  Scores: [0.506353494993945, 0.4579505105103895, 0.43134832916584803, 0.49161771686915634, 0.5088726821608882, 0.46004002939364014]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [48/49] Elapsed 1m 16s (remain 0m 0s) Loss: 1.2915(1.3270) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 8 result ==========\n",
      "Score: 0.4732  Scores: [0.4942120236261712, 0.4586480745255393, 0.42687827766076236, 0.4877003817961311, 0.5120042359159814, 0.46004002939364014]\n",
      "========== fold: 9 training ==========\n",
      "DebertaV2Config {\n",
      "  \"_name_or_path\": \"microsoft/deberta-v3-large\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-07,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_relative_positions\": -1,\n",
      "  \"model_type\": \"deberta-v2\",\n",
      "  \"norm_rel_ebd\": \"layer_norm\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pooler_dropout\": 0,\n",
      "  \"pooler_hidden_act\": \"gelu\",\n",
      "  \"pooler_hidden_size\": 1024,\n",
      "  \"pos_att_type\": [\n",
      "    \"p2c\",\n",
      "    \"c2p\"\n",
      "  ],\n",
      "  \"position_biased_input\": false,\n",
      "  \"position_buckets\": 256,\n",
      "  \"relative_attention\": true,\n",
      "  \"share_att_key\": true,\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 0,\n",
      "  \"vocab_size\": 128100\n",
      "}\n",
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/879] Elapsed 0m 2s (remain 32m 44s) Loss: 2.2131(2.2131) Grad: inf  LR: 0.00002000  \n",
      "Epoch: [1][20/879] Elapsed 0m 41s (remain 28m 17s) Loss: 1.9907(2.0391) Grad: 165777.3281  LR: 0.00002000  \n",
      "Epoch: [1][40/879] Elapsed 1m 31s (remain 31m 4s) Loss: 1.5570(1.8601) Grad: 109265.7266  LR: 0.00001999  \n",
      "Epoch: [1][60/879] Elapsed 2m 27s (remain 32m 53s) Loss: 1.7328(1.8207) Grad: 108986.1250  LR: 0.00001999  \n",
      "Epoch: [1][80/879] Elapsed 3m 25s (remain 33m 49s) Loss: 1.4259(1.7656) Grad: 161328.7188  LR: 0.00001997  \n",
      "Epoch: [1][100/879] Elapsed 4m 12s (remain 32m 21s) Loss: 1.5015(1.7223) Grad: 153742.5000  LR: 0.00001996  \n",
      "Epoch: [1][120/879] Elapsed 4m 58s (remain 31m 10s) Loss: 1.5362(1.6811) Grad: 180003.7031  LR: 0.00001994  \n",
      "Epoch: [1][140/879] Elapsed 5m 45s (remain 30m 9s) Loss: 1.5213(1.6500) Grad: 171148.1875  LR: 0.00001992  \n",
      "Epoch: [1][160/879] Elapsed 6m 36s (remain 29m 29s) Loss: 1.7004(1.6264) Grad: 188530.1875  LR: 0.00001990  \n",
      "Epoch: [1][180/879] Elapsed 7m 23s (remain 28m 31s) Loss: 1.1850(1.6157) Grad: 97173.0078  LR: 0.00001987  \n",
      "Epoch: [1][200/879] Elapsed 8m 3s (remain 27m 9s) Loss: 1.2456(1.6040) Grad: 137202.5156  LR: 0.00001984  \n",
      "Epoch: [1][220/879] Elapsed 8m 47s (remain 26m 11s) Loss: 1.5995(1.5925) Grad: 99269.9766  LR: 0.00001981  \n",
      "Epoch: [1][240/879] Elapsed 9m 29s (remain 25m 7s) Loss: 1.3489(1.5748) Grad: 130649.6797  LR: 0.00001977  \n",
      "Epoch: [1][260/879] Elapsed 10m 13s (remain 24m 13s) Loss: 1.3896(1.5664) Grad: 111661.6328  LR: 0.00001973  \n",
      "Epoch: [1][280/879] Elapsed 11m 2s (remain 23m 30s) Loss: 1.2933(1.5564) Grad: 115224.3516  LR: 0.00001969  \n",
      "Epoch: [1][300/879] Elapsed 11m 54s (remain 22m 52s) Loss: 1.2817(1.5491) Grad: 107736.3047  LR: 0.00001964  \n",
      "Epoch: [1][320/879] Elapsed 12m 44s (remain 22m 9s) Loss: 1.9386(1.5432) Grad: 186547.1562  LR: 0.00001959  \n",
      "Epoch: [1][340/879] Elapsed 13m 34s (remain 21m 25s) Loss: 1.4701(1.5392) Grad: 150509.7969  LR: 0.00001954  \n",
      "Epoch: [1][360/879] Elapsed 14m 28s (remain 20m 45s) Loss: 1.0962(1.5302) Grad: 81318.2812  LR: 0.00001949  \n",
      "Epoch: [1][380/879] Elapsed 15m 10s (remain 19m 50s) Loss: 1.3081(1.5261) Grad: 105829.2734  LR: 0.00001943  \n",
      "Epoch: [1][400/879] Elapsed 15m 57s (remain 19m 1s) Loss: 1.1626(1.5216) Grad: 84165.9922  LR: 0.00001937  \n",
      "Epoch: [1][420/879] Elapsed 16m 44s (remain 18m 12s) Loss: 1.1386(1.5140) Grad: 124447.2812  LR: 0.00001930  \n",
      "Epoch: [1][440/879] Elapsed 17m 27s (remain 17m 20s) Loss: 1.5636(1.5077) Grad: 167253.8594  LR: 0.00001923  \n",
      "Epoch: [1][460/879] Elapsed 18m 13s (remain 16m 31s) Loss: 1.8607(1.5050) Grad: 180662.8594  LR: 0.00001916  \n",
      "Epoch: [1][480/879] Elapsed 19m 6s (remain 15m 48s) Loss: 1.2012(1.5014) Grad: 130972.5391  LR: 0.00001909  \n",
      "Epoch: [1][500/879] Elapsed 19m 53s (remain 15m 0s) Loss: 1.4991(1.4969) Grad: 128989.6016  LR: 0.00001902  \n",
      "Epoch: [1][520/879] Elapsed 20m 42s (remain 14m 13s) Loss: 1.6660(1.4937) Grad: 126144.7031  LR: 0.00001894  \n",
      "Epoch: [1][540/879] Elapsed 21m 34s (remain 13m 28s) Loss: 1.2824(1.4915) Grad: 110304.8047  LR: 0.00001886  \n",
      "Epoch: [1][560/879] Elapsed 22m 22s (remain 12m 41s) Loss: 1.2983(1.4872) Grad: 94166.3750  LR: 0.00001877  \n",
      "Epoch: [1][580/879] Elapsed 23m 11s (remain 11m 53s) Loss: 1.4213(1.4830) Grad: 109905.3672  LR: 0.00001868  \n",
      "Epoch: [1][600/879] Elapsed 24m 2s (remain 11m 7s) Loss: 1.5562(1.4807) Grad: 142884.7812  LR: 0.00001859  \n",
      "Epoch: [1][620/879] Elapsed 24m 48s (remain 10m 18s) Loss: 1.1849(1.4775) Grad: 81566.4219  LR: 0.00001850  \n",
      "Epoch: [1][640/879] Elapsed 25m 33s (remain 9m 29s) Loss: 1.3520(1.4758) Grad: 112132.4922  LR: 0.00001841  \n",
      "Epoch: [1][660/879] Elapsed 26m 19s (remain 8m 40s) Loss: 1.1755(1.4754) Grad: 80230.9297  LR: 0.00001831  \n",
      "Epoch: [1][680/879] Elapsed 27m 11s (remain 7m 54s) Loss: 1.2814(1.4716) Grad: 125079.5781  LR: 0.00001821  \n",
      "Epoch: [1][700/879] Elapsed 27m 56s (remain 7m 5s) Loss: 1.3778(1.4708) Grad: 96446.8125  LR: 0.00001810  \n",
      "Epoch: [1][720/879] Elapsed 28m 40s (remain 6m 17s) Loss: 1.2546(1.4686) Grad: 82727.8359  LR: 0.00001800  \n",
      "Epoch: [1][740/879] Elapsed 29m 38s (remain 5m 31s) Loss: 1.3052(1.4657) Grad: 129272.8984  LR: 0.00001789  \n",
      "Epoch: [1][760/879] Elapsed 30m 29s (remain 4m 43s) Loss: 1.6163(1.4652) Grad: 135453.2188  LR: 0.00001778  \n",
      "Epoch: [1][780/879] Elapsed 31m 13s (remain 3m 55s) Loss: 1.1745(1.4627) Grad: 100280.6406  LR: 0.00001767  \n",
      "Epoch: [1][800/879] Elapsed 31m 51s (remain 3m 6s) Loss: 1.8083(1.4615) Grad: 144039.5938  LR: 0.00001755  \n",
      "Epoch: [1][820/879] Elapsed 32m 40s (remain 2m 18s) Loss: 1.3920(1.4603) Grad: 126396.3125  LR: 0.00001743  \n",
      "Epoch: [1][840/879] Elapsed 33m 28s (remain 1m 30s) Loss: 1.2100(1.4584) Grad: 91541.3125  LR: 0.00001731  \n",
      "Epoch: [1][860/879] Elapsed 34m 11s (remain 0m 42s) Loss: 1.1223(1.4562) Grad: 92016.8750  LR: 0.00001719  \n",
      "Epoch: [1][878/879] Elapsed 34m 49s (remain 0m 0s) Loss: 1.3335(1.4539) Grad: 130184.4688  LR: 0.00001708  \n",
      "EVAL: [0/49] Elapsed 0m 3s (remain 2m 45s) Loss: 1.2101(1.2101) \n",
      "EVAL: [20/49] Elapsed 0m 36s (remain 0m 49s) Loss: 1.3496(1.4061) \n",
      "EVAL: [40/49] Elapsed 1m 3s (remain 0m 12s) Loss: 1.2026(1.3758) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 1.4539  avg_val_loss: 1.3754  time: 2165s\n",
      "Epoch 1 - Score: 0.4986  Scores: [0.549234160686775, 0.46974808707021093, 0.46153028511857447, 0.5069667702349425, 0.5363091934886145, 0.46770717334674267]\n",
      "Epoch 1 - Save Best Score: 0.4986 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [48/49] Elapsed 1m 15s (remain 0m 0s) Loss: 1.2330(1.3754) \n",
      "Epoch: [2][0/879] Elapsed 0m 2s (remain 36m 0s) Loss: 1.2122(1.2122) Grad: 388913.8438  LR: 0.00001707  \n",
      "Epoch: [2][20/879] Elapsed 0m 50s (remain 34m 27s) Loss: 1.2816(1.2760) Grad: 600609.8125  LR: 0.00001694  \n",
      "Epoch: [2][40/879] Elapsed 1m 37s (remain 33m 11s) Loss: 1.3148(1.2930) Grad: 503381.3438  LR: 0.00001681  \n",
      "Epoch: [2][60/879] Elapsed 2m 24s (remain 32m 22s) Loss: 1.2374(1.2910) Grad: 455771.2188  LR: 0.00001668  \n",
      "Epoch: [2][80/879] Elapsed 3m 9s (remain 31m 8s) Loss: 1.5517(1.2901) Grad: 388182.6250  LR: 0.00001655  \n",
      "Epoch: [2][100/879] Elapsed 4m 0s (remain 30m 52s) Loss: 1.2023(1.2834) Grad: 372138.1562  LR: 0.00001641  \n",
      "Epoch: [2][120/879] Elapsed 4m 56s (remain 30m 54s) Loss: 1.2072(1.2758) Grad: 337023.2188  LR: 0.00001627  \n",
      "Epoch: [2][140/879] Elapsed 5m 42s (remain 29m 52s) Loss: 1.1718(1.2686) Grad: 404224.3125  LR: 0.00001613  \n",
      "Epoch: [2][160/879] Elapsed 6m 27s (remain 28m 48s) Loss: 1.0935(1.2696) Grad: 418206.9062  LR: 0.00001599  \n",
      "Epoch: [2][180/879] Elapsed 7m 10s (remain 27m 41s) Loss: 1.2922(1.2712) Grad: 523064.5625  LR: 0.00001585  \n",
      "Epoch: [2][200/879] Elapsed 7m 58s (remain 26m 52s) Loss: 1.1026(1.2699) Grad: 366988.8125  LR: 0.00001570  \n",
      "Epoch: [2][220/879] Elapsed 8m 38s (remain 25m 43s) Loss: 1.2238(1.2722) Grad: 368122.8125  LR: 0.00001555  \n",
      "Epoch: [2][240/879] Elapsed 9m 29s (remain 25m 6s) Loss: 1.1750(1.2698) Grad: 343106.1250  LR: 0.00001540  \n",
      "Epoch: [2][260/879] Elapsed 10m 14s (remain 24m 15s) Loss: 1.2335(1.2706) Grad: 525685.3750  LR: 0.00001525  \n",
      "Epoch: [2][280/879] Elapsed 10m 58s (remain 23m 21s) Loss: 1.3558(1.2711) Grad: 211002.3750  LR: 0.00001510  \n",
      "Epoch: [2][300/879] Elapsed 11m 43s (remain 22m 30s) Loss: 1.1212(1.2670) Grad: 201763.0781  LR: 0.00001495  \n",
      "Epoch: [2][320/879] Elapsed 12m 25s (remain 21m 35s) Loss: 1.2758(1.2646) Grad: 244969.2656  LR: 0.00001479  \n",
      "Epoch: [2][340/879] Elapsed 13m 8s (remain 20m 43s) Loss: 1.2494(1.2616) Grad: 169502.3906  LR: 0.00001463  \n",
      "Epoch: [2][360/879] Elapsed 14m 0s (remain 20m 5s) Loss: 1.0274(1.2597) Grad: 172813.2500  LR: 0.00001447  \n",
      "Epoch: [2][380/879] Elapsed 14m 39s (remain 19m 9s) Loss: 1.0028(1.2553) Grad: 169383.0625  LR: 0.00001431  \n",
      "Epoch: [2][400/879] Elapsed 15m 26s (remain 18m 24s) Loss: 1.2633(1.2526) Grad: 208666.8281  LR: 0.00001415  \n",
      "Epoch: [2][420/879] Elapsed 16m 20s (remain 17m 46s) Loss: 1.0950(1.2524) Grad: 180047.8906  LR: 0.00001399  \n",
      "Epoch: [2][440/879] Elapsed 17m 3s (remain 16m 56s) Loss: 1.3177(1.2507) Grad: 186545.3594  LR: 0.00001382  \n",
      "Epoch: [2][460/879] Elapsed 17m 53s (remain 16m 13s) Loss: 1.8713(1.2518) Grad: 574938.1875  LR: 0.00001366  \n",
      "Epoch: [2][480/879] Elapsed 18m 38s (remain 15m 25s) Loss: 1.3361(1.2539) Grad: 305786.9062  LR: 0.00001349  \n",
      "Epoch: [2][500/879] Elapsed 19m 28s (remain 14m 41s) Loss: 1.7443(1.2546) Grad: 345991.5000  LR: 0.00001332  \n",
      "Epoch: [2][520/879] Elapsed 20m 14s (remain 13m 54s) Loss: 1.0725(1.2536) Grad: 190592.8125  LR: 0.00001315  \n",
      "Epoch: [2][540/879] Elapsed 21m 3s (remain 13m 9s) Loss: 1.3420(1.2542) Grad: 262532.8125  LR: 0.00001298  \n",
      "Epoch: [2][560/879] Elapsed 21m 57s (remain 12m 26s) Loss: 1.2668(1.2553) Grad: 239336.1562  LR: 0.00001281  \n",
      "Epoch: [2][580/879] Elapsed 22m 40s (remain 11m 37s) Loss: 1.3397(1.2545) Grad: 309384.3750  LR: 0.00001264  \n",
      "Epoch: [2][600/879] Elapsed 23m 24s (remain 10m 49s) Loss: 1.4824(1.2536) Grad: 282310.5625  LR: 0.00001247  \n",
      "Epoch: [2][620/879] Elapsed 24m 15s (remain 10m 4s) Loss: 1.2988(1.2521) Grad: 232129.8281  LR: 0.00001230  \n",
      "Epoch: [2][640/879] Elapsed 24m 54s (remain 9m 14s) Loss: 1.1555(1.2510) Grad: 232120.5469  LR: 0.00001212  \n",
      "Epoch: [2][660/879] Elapsed 25m 42s (remain 8m 28s) Loss: 1.0201(1.2493) Grad: 198428.2188  LR: 0.00001195  \n",
      "Epoch: [2][680/879] Elapsed 26m 33s (remain 7m 43s) Loss: 2.1268(1.2503) Grad: 346159.1250  LR: 0.00001177  \n",
      "Epoch: [2][700/879] Elapsed 27m 22s (remain 6m 57s) Loss: 0.9934(1.2504) Grad: 187572.7812  LR: 0.00001160  \n",
      "Epoch: [2][720/879] Elapsed 28m 21s (remain 6m 12s) Loss: 1.2593(1.2500) Grad: 209114.4844  LR: 0.00001142  \n",
      "Epoch: [2][740/879] Elapsed 29m 14s (remain 5m 26s) Loss: 1.0811(1.2500) Grad: 259789.7500  LR: 0.00001124  \n",
      "Epoch: [2][760/879] Elapsed 30m 7s (remain 4m 40s) Loss: 1.0800(1.2499) Grad: 240073.4062  LR: 0.00001106  \n",
      "Epoch: [2][780/879] Elapsed 30m 49s (remain 3m 52s) Loss: 1.0059(1.2478) Grad: 213954.1094  LR: 0.00001089  \n",
      "Epoch: [2][800/879] Elapsed 31m 37s (remain 3m 4s) Loss: 1.0827(1.2481) Grad: 190267.3594  LR: 0.00001071  \n",
      "Epoch: [2][820/879] Elapsed 32m 26s (remain 2m 17s) Loss: 1.0284(1.2489) Grad: 203833.9844  LR: 0.00001053  \n",
      "Epoch: [2][840/879] Elapsed 33m 14s (remain 1m 30s) Loss: 1.1553(1.2498) Grad: 210638.3594  LR: 0.00001035  \n",
      "Epoch: [2][860/879] Elapsed 33m 58s (remain 0m 42s) Loss: 1.0576(1.2483) Grad: 226585.8750  LR: 0.00001017  \n",
      "Epoch: [2][878/879] Elapsed 34m 50s (remain 0m 0s) Loss: 1.1144(1.2476) Grad: 254817.2656  LR: 0.00001001  \n",
      "EVAL: [0/49] Elapsed 0m 3s (remain 2m 41s) Loss: 1.2233(1.2233) \n",
      "EVAL: [20/49] Elapsed 0m 36s (remain 0m 48s) Loss: 1.4112(1.3880) \n",
      "EVAL: [40/49] Elapsed 1m 2s (remain 0m 12s) Loss: 1.3405(1.3427) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 1.2476  avg_val_loss: 1.3506  time: 2166s\n",
      "Epoch 2 - Score: 0.4932  Scores: [0.5515516146957321, 0.4711037842240328, 0.4446392713567405, 0.4974424384708614, 0.5309310266899466, 0.46359839234811323]\n",
      "Epoch 2 - Save Best Score: 0.4932 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [48/49] Elapsed 1m 15s (remain 0m 0s) Loss: 1.2405(1.3506) \n",
      "Epoch: [3][0/879] Elapsed 0m 5s (remain 80m 15s) Loss: 1.0194(1.0194) Grad: 612480.7500  LR: 0.00001000  \n",
      "Epoch: [3][20/879] Elapsed 0m 58s (remain 39m 37s) Loss: 1.0248(1.1459) Grad: 454729.1250  LR: 0.00000983  \n",
      "Epoch: [3][40/879] Elapsed 1m 47s (remain 36m 27s) Loss: 1.0762(1.1292) Grad: 354418.5000  LR: 0.00000965  \n",
      "Epoch: [3][60/879] Elapsed 2m 31s (remain 33m 53s) Loss: 1.5877(1.1479) Grad: 770199.5000  LR: 0.00000947  \n",
      "Epoch: [3][80/879] Elapsed 3m 22s (remain 33m 17s) Loss: 0.8865(1.1441) Grad: 404962.9062  LR: 0.00000929  \n",
      "Epoch: [3][100/879] Elapsed 4m 15s (remain 32m 50s) Loss: 0.9068(1.1369) Grad: 406316.0938  LR: 0.00000911  \n",
      "Epoch: [3][120/879] Elapsed 5m 6s (remain 32m 2s) Loss: 1.0504(1.1347) Grad: 394758.9375  LR: 0.00000894  \n",
      "Epoch: [3][140/879] Elapsed 5m 50s (remain 30m 34s) Loss: 1.2497(1.1402) Grad: 421014.8750  LR: 0.00000876  \n",
      "Epoch: [3][160/879] Elapsed 6m 46s (remain 30m 12s) Loss: 1.0825(1.1397) Grad: 468513.9375  LR: 0.00000858  \n",
      "Epoch: [3][180/879] Elapsed 7m 29s (remain 28m 52s) Loss: 1.0542(1.1418) Grad: 398030.0625  LR: 0.00000840  \n",
      "Epoch: [3][200/879] Elapsed 8m 19s (remain 28m 4s) Loss: 1.3038(1.1411) Grad: 633705.7500  LR: 0.00000823  \n",
      "Epoch: [3][220/879] Elapsed 9m 3s (remain 26m 59s) Loss: 0.9942(1.1424) Grad: 364273.6875  LR: 0.00000805  \n",
      "Epoch: [3][240/879] Elapsed 9m 46s (remain 25m 53s) Loss: 1.0467(1.1438) Grad: 397585.0312  LR: 0.00000788  \n",
      "Epoch: [3][260/879] Elapsed 10m 39s (remain 25m 13s) Loss: 1.0343(1.1423) Grad: 457398.0938  LR: 0.00000770  \n",
      "Epoch: [3][280/879] Elapsed 11m 27s (remain 24m 23s) Loss: 1.0168(1.1452) Grad: 423848.7812  LR: 0.00000753  \n",
      "Epoch: [3][300/879] Elapsed 12m 19s (remain 23m 40s) Loss: 1.3409(1.1414) Grad: 661254.3125  LR: 0.00000736  \n",
      "Epoch: [3][320/879] Elapsed 13m 6s (remain 22m 47s) Loss: 1.1500(1.1385) Grad: 461532.1250  LR: 0.00000719  \n",
      "Epoch: [3][340/879] Elapsed 13m 53s (remain 21m 54s) Loss: 1.1573(1.1387) Grad: 521454.5625  LR: 0.00000702  \n",
      "Epoch: [3][360/879] Elapsed 14m 37s (remain 20m 59s) Loss: 1.0372(1.1366) Grad: 513464.5000  LR: 0.00000685  \n",
      "Epoch: [3][380/879] Elapsed 15m 22s (remain 20m 5s) Loss: 1.0971(1.1335) Grad: 586108.0000  LR: 0.00000668  \n",
      "Epoch: [3][400/879] Elapsed 16m 8s (remain 19m 14s) Loss: 0.9250(1.1321) Grad: 457674.6875  LR: 0.00000651  \n",
      "Epoch: [3][420/879] Elapsed 16m 58s (remain 18m 27s) Loss: 1.1166(1.1341) Grad: 576802.8750  LR: 0.00000634  \n",
      "Epoch: [3][440/879] Elapsed 17m 45s (remain 17m 38s) Loss: 1.0806(1.1347) Grad: 401757.3438  LR: 0.00000618  \n",
      "Epoch: [3][460/879] Elapsed 18m 37s (remain 16m 53s) Loss: 0.9845(1.1357) Grad: 364889.5625  LR: 0.00000601  \n",
      "Epoch: [3][480/879] Elapsed 19m 18s (remain 15m 58s) Loss: 1.0661(1.1347) Grad: 434839.3750  LR: 0.00000585  \n",
      "Epoch: [3][500/879] Elapsed 19m 57s (remain 15m 3s) Loss: 1.0360(1.1343) Grad: 512812.9062  LR: 0.00000569  \n",
      "Epoch: [3][520/879] Elapsed 20m 45s (remain 14m 15s) Loss: 1.2669(1.1355) Grad: 606934.3750  LR: 0.00000553  \n",
      "Epoch: [3][540/879] Elapsed 21m 35s (remain 13m 29s) Loss: 0.8287(1.1354) Grad: 382148.9688  LR: 0.00000537  \n",
      "Epoch: [3][560/879] Elapsed 22m 24s (remain 12m 41s) Loss: 1.3757(1.1347) Grad: 665914.0000  LR: 0.00000521  \n",
      "Epoch: [3][580/879] Elapsed 23m 19s (remain 11m 58s) Loss: 1.0054(1.1327) Grad: 447514.0625  LR: 0.00000505  \n",
      "Epoch: [3][600/879] Elapsed 23m 59s (remain 11m 5s) Loss: 1.0642(1.1325) Grad: 506824.3125  LR: 0.00000490  \n",
      "Epoch: [3][620/879] Elapsed 24m 46s (remain 10m 17s) Loss: 1.0525(1.1308) Grad: 548674.1250  LR: 0.00000475  \n",
      "Epoch: [3][640/879] Elapsed 25m 35s (remain 9m 30s) Loss: 1.0418(1.1297) Grad: 476391.5938  LR: 0.00000460  \n",
      "Epoch: [3][660/879] Elapsed 26m 24s (remain 8m 42s) Loss: 0.9453(1.1279) Grad: 463383.1875  LR: 0.00000445  \n",
      "Epoch: [3][680/879] Elapsed 27m 9s (remain 7m 53s) Loss: 1.2331(1.1274) Grad: 649810.3750  LR: 0.00000430  \n",
      "Epoch: [3][700/879] Elapsed 28m 0s (remain 7m 6s) Loss: 1.1449(1.1265) Grad: 504565.0312  LR: 0.00000415  \n",
      "Epoch: [3][720/879] Elapsed 28m 51s (remain 6m 19s) Loss: 1.1356(1.1282) Grad: 376143.9688  LR: 0.00000401  \n",
      "Epoch: [3][740/879] Elapsed 29m 36s (remain 5m 30s) Loss: 1.2054(1.1264) Grad: 728647.2500  LR: 0.00000387  \n",
      "Epoch: [3][760/879] Elapsed 30m 25s (remain 4m 43s) Loss: 1.1164(1.1262) Grad: 480458.2812  LR: 0.00000373  \n",
      "Epoch: [3][780/879] Elapsed 31m 17s (remain 3m 55s) Loss: 1.3418(1.1269) Grad: 726737.3125  LR: 0.00000359  \n",
      "Epoch: [3][800/879] Elapsed 32m 6s (remain 3m 7s) Loss: 1.7076(1.1274) Grad: 1319778.2500  LR: 0.00000345  \n",
      "Epoch: [3][820/879] Elapsed 32m 50s (remain 2m 19s) Loss: 0.9963(1.1259) Grad: 629382.3125  LR: 0.00000332  \n",
      "Epoch: [3][840/879] Elapsed 33m 42s (remain 1m 31s) Loss: 1.1062(1.1254) Grad: 501100.6562  LR: 0.00000319  \n",
      "Epoch: [3][860/879] Elapsed 34m 25s (remain 0m 43s) Loss: 1.0291(1.1258) Grad: 612389.8750  LR: 0.00000306  \n",
      "Epoch: [3][878/879] Elapsed 35m 9s (remain 0m 0s) Loss: 1.1160(1.1261) Grad: 558052.4375  LR: 0.00000294  \n",
      "EVAL: [0/49] Elapsed 0m 3s (remain 2m 41s) Loss: 1.2209(1.2209) \n",
      "EVAL: [20/49] Elapsed 0m 36s (remain 0m 48s) Loss: 1.5036(1.4090) \n",
      "EVAL: [40/49] Elapsed 1m 2s (remain 0m 12s) Loss: 1.3724(1.3607) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 1.1261  avg_val_loss: 1.3641  time: 2185s\n",
      "Epoch 3 - Score: 0.4944  Scores: [0.544569667213461, 0.47447608055665796, 0.44607128559988557, 0.5050762722761054, 0.5351187165632749, 0.4608388535915693]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [48/49] Elapsed 1m 14s (remain 0m 0s) Loss: 1.2726(1.3641) \n",
      "Epoch: [4][0/879] Elapsed 0m 1s (remain 25m 55s) Loss: 0.8878(0.8878) Grad: 368395.0938  LR: 0.00000294  \n",
      "Epoch: [4][20/879] Elapsed 0m 52s (remain 36m 4s) Loss: 1.0523(1.0633) Grad: 406275.0938  LR: 0.00000281  \n",
      "Epoch: [4][40/879] Elapsed 1m 34s (remain 32m 9s) Loss: 0.9900(1.0786) Grad: 349812.1562  LR: 0.00000269  \n",
      "Epoch: [4][60/879] Elapsed 2m 25s (remain 32m 27s) Loss: 0.8096(1.0638) Grad: 453989.0312  LR: 0.00000257  \n",
      "Epoch: [4][80/879] Elapsed 3m 19s (remain 32m 41s) Loss: 0.9855(1.0689) Grad: 408767.6875  LR: 0.00000245  \n",
      "Epoch: [4][100/879] Elapsed 4m 3s (remain 31m 15s) Loss: 0.9962(1.0605) Grad: 425501.5312  LR: 0.00000233  \n",
      "Epoch: [4][120/879] Elapsed 4m 58s (remain 31m 7s) Loss: 0.8281(1.0594) Grad: 536965.8125  LR: 0.00000222  \n",
      "Epoch: [4][140/879] Elapsed 5m 38s (remain 29m 34s) Loss: 1.1025(1.0588) Grad: 554659.6250  LR: 0.00000211  \n",
      "Epoch: [4][160/879] Elapsed 6m 25s (remain 28m 37s) Loss: 0.9248(1.0604) Grad: 472021.3125  LR: 0.00000200  \n",
      "Epoch: [4][180/879] Elapsed 7m 18s (remain 28m 11s) Loss: 1.0878(1.0605) Grad: 511637.5000  LR: 0.00000190  \n",
      "Epoch: [4][200/879] Elapsed 8m 9s (remain 27m 31s) Loss: 0.9603(1.0644) Grad: 415571.1250  LR: 0.00000179  \n",
      "Epoch: [4][220/879] Elapsed 8m 59s (remain 26m 47s) Loss: 0.8779(1.0631) Grad: 387245.8750  LR: 0.00000169  \n",
      "Epoch: [4][240/879] Elapsed 9m 52s (remain 26m 8s) Loss: 1.0769(1.0624) Grad: 384271.1875  LR: 0.00000159  \n",
      "Epoch: [4][260/879] Elapsed 10m 38s (remain 25m 10s) Loss: 1.1038(1.0614) Grad: 554450.6875  LR: 0.00000150  \n",
      "Epoch: [4][280/879] Elapsed 11m 25s (remain 24m 18s) Loss: 1.0182(1.0615) Grad: 446212.9688  LR: 0.00000141  \n",
      "Epoch: [4][300/879] Elapsed 12m 10s (remain 23m 22s) Loss: 1.0813(1.0625) Grad: 573816.6875  LR: 0.00000132  \n",
      "Epoch: [4][320/879] Elapsed 12m 52s (remain 22m 22s) Loss: 0.9924(1.0590) Grad: 461572.6562  LR: 0.00000123  \n",
      "Epoch: [4][340/879] Elapsed 13m 32s (remain 21m 21s) Loss: 0.9162(1.0580) Grad: 388514.1875  LR: 0.00000114  \n",
      "Epoch: [4][360/879] Elapsed 14m 10s (remain 20m 20s) Loss: 1.0068(1.0563) Grad: 527143.2500  LR: 0.00000106  \n",
      "Epoch: [4][380/879] Elapsed 14m 59s (remain 19m 35s) Loss: 0.8369(1.0562) Grad: 484930.7500  LR: 0.00000098  \n",
      "Epoch: [4][400/879] Elapsed 15m 42s (remain 18m 43s) Loss: 0.8582(1.0571) Grad: 470301.8125  LR: 0.00000091  \n",
      "Epoch: [4][420/879] Elapsed 16m 29s (remain 17m 56s) Loss: 1.0880(1.0580) Grad: 410280.5625  LR: 0.00000084  \n",
      "Epoch: [4][440/879] Elapsed 17m 14s (remain 17m 7s) Loss: 1.3306(1.0584) Grad: 669809.3125  LR: 0.00000077  \n",
      "Epoch: [4][460/879] Elapsed 18m 3s (remain 16m 22s) Loss: 1.2642(1.0587) Grad: 637662.5000  LR: 0.00000070  \n",
      "Epoch: [4][480/879] Elapsed 18m 48s (remain 15m 33s) Loss: 1.0512(1.0577) Grad: 371967.8750  LR: 0.00000063  \n",
      "Epoch: [4][500/879] Elapsed 19m 31s (remain 14m 44s) Loss: 0.8538(1.0557) Grad: 505973.0625  LR: 0.00000057  \n",
      "Epoch: [4][520/879] Elapsed 20m 20s (remain 13m 58s) Loss: 1.2968(1.0549) Grad: 686993.7500  LR: 0.00000051  \n",
      "Epoch: [4][540/879] Elapsed 21m 12s (remain 13m 15s) Loss: 1.1221(1.0546) Grad: 518746.6562  LR: 0.00000046  \n",
      "Epoch: [4][560/879] Elapsed 22m 12s (remain 12m 35s) Loss: 1.0574(1.0536) Grad: 419147.3125  LR: 0.00000041  \n",
      "Epoch: [4][580/879] Elapsed 22m 58s (remain 11m 47s) Loss: 1.2773(1.0546) Grad: 491659.6562  LR: 0.00000036  \n",
      "Epoch: [4][600/879] Elapsed 23m 49s (remain 11m 1s) Loss: 1.1305(1.0562) Grad: 350959.8438  LR: 0.00000031  \n",
      "Epoch: [4][620/879] Elapsed 24m 40s (remain 10m 15s) Loss: 1.3766(1.0581) Grad: 711160.6250  LR: 0.00000027  \n",
      "Epoch: [4][640/879] Elapsed 25m 28s (remain 9m 27s) Loss: 0.9554(1.0568) Grad: 414874.8438  LR: 0.00000023  \n",
      "Epoch: [4][660/879] Elapsed 26m 9s (remain 8m 37s) Loss: 1.0999(1.0564) Grad: 456063.4062  LR: 0.00000019  \n",
      "Epoch: [4][680/879] Elapsed 27m 3s (remain 7m 51s) Loss: 1.0673(1.0579) Grad: 425495.0312  LR: 0.00000016  \n",
      "Epoch: [4][700/879] Elapsed 27m 46s (remain 7m 3s) Loss: 1.1658(1.0561) Grad: 478706.1250  LR: 0.00000013  \n",
      "Epoch: [4][720/879] Elapsed 28m 35s (remain 6m 16s) Loss: 1.1451(1.0573) Grad: 813983.0000  LR: 0.00000010  \n",
      "Epoch: [4][740/879] Elapsed 29m 28s (remain 5m 29s) Loss: 0.9981(1.0577) Grad: 542226.7500  LR: 0.00000008  \n",
      "Epoch: [4][760/879] Elapsed 30m 14s (remain 4m 41s) Loss: 1.1376(1.0574) Grad: 656478.3125  LR: 0.00000006  \n",
      "Epoch: [4][780/879] Elapsed 31m 4s (remain 3m 53s) Loss: 0.9317(1.0576) Grad: 358081.7500  LR: 0.00000004  \n",
      "Epoch: [4][800/879] Elapsed 31m 51s (remain 3m 6s) Loss: 1.1320(1.0563) Grad: 506316.0312  LR: 0.00000003  \n",
      "Epoch: [4][820/879] Elapsed 32m 40s (remain 2m 18s) Loss: 1.2218(1.0561) Grad: 624724.6875  LR: 0.00000001  \n",
      "Epoch: [4][840/879] Elapsed 33m 31s (remain 1m 30s) Loss: 1.0340(1.0555) Grad: 462326.9375  LR: 0.00000001  \n",
      "Epoch: [4][860/879] Elapsed 34m 16s (remain 0m 42s) Loss: 1.2921(1.0556) Grad: 1303840.0000  LR: 0.00000000  \n",
      "Epoch: [4][878/879] Elapsed 34m 55s (remain 0m 0s) Loss: 1.0076(1.0559) Grad: 469334.3750  LR: 0.00000000  \n",
      "EVAL: [0/49] Elapsed 0m 3s (remain 2m 41s) Loss: 1.2319(1.2319) \n",
      "EVAL: [20/49] Elapsed 0m 36s (remain 0m 48s) Loss: 1.5219(1.4150) \n",
      "EVAL: [40/49] Elapsed 1m 2s (remain 0m 12s) Loss: 1.3805(1.3662) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 1.0559  avg_val_loss: 1.3729  time: 2170s\n",
      "Epoch 4 - Score: 0.4947  Scores: [0.5555838995037159, 0.47514766770178823, 0.4417613170304636, 0.5082231953842038, 0.5333280186810028, 0.4538666081121012]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVAL: [48/49] Elapsed 1m 15s (remain 0m 0s) Loss: 1.3088(1.3729) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 9 result ==========\n",
      "Score: 0.4932  Scores: [0.5515516146957321, 0.4711037842240328, 0.4446392713567405, 0.4974424384708614, 0.5309310266899466, 0.46359839234811323]\n",
      "========== CV ==========\n",
      "Score: 0.4879  Scores: [0.5157270992439399, 0.46798459874715614, 0.4534214974095285, 0.4996804090437293, 0.5118406301486104, 0.4787830686616442]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def get_result(oof_df):\n",
    "    labels = oof_df[CFG.target_cols].values\n",
    "    preds = oof_df[[f\"pred_{c}\" for c in CFG.target_cols]].values\n",
    "    score, scores = get_score(labels, preds)\n",
    "    LOGGER.info(f'Score: {score:<.4f}  Scores: {scores}')\n",
    "\n",
    "if CFG.train:\n",
    "    oof_df = pd.DataFrame()\n",
    "    for fold in range(6, CFG.n_fold):\n",
    "        if fold in CFG.trn_fold:\n",
    "            _oof_df = train_loop(train, fold)\n",
    "            oof_df = pd.concat([oof_df, _oof_df])\n",
    "            LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "            get_result(_oof_df)\n",
    "    oof_df = oof_df.reset_index(drop=True)\n",
    "    LOGGER.info(f\"========== CV ==========\")\n",
    "    get_result(oof_df)\n",
    "    oof_df.to_pickle(OUTPUT_DIR+'oof_df.pkl')\n",
    "\n",
    "if CFG.wandb:\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
